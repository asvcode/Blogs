{
  
    
        "post0": {
            "title": "Medical Imaging Starter",
            "content": "Goal: . The goals of this starter notebook are to: . use DICOMs as the image input | high level overview of what considerations need to be taken and what the results mean when creating a model that predicts medical conditions | . The dataset used is conveniently provided by fastai - SIIM-ACR Pneumothorax Segmentation dataset and contains 250 Dicom images (175 No Pneumothorax and 75 Pneumothorax) . Considerations: . patient overlap | sampling | evaluting AI models for medical use | . This notebook is based on this fastai notebook. For more information about DICOMs and fastai medical imaging you can click here . Load the Data . pneumothorax_source = untar_data(URLs.SIIM_SMALL) items = get_dicom_files(pneumothorax_source, recurse=True, folders=&#39;sm&#39;) df = pd.read_csv(pneumothorax_source/f&quot;labels_sm.csv&quot;) . items . (#26) [Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000000 - Copy.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000000.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000002.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000005.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000006 - Copy.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000006.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000007.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000008.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000009.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000011.dcm&#39;)...] . Side Note: The SIIM_SMALL dataset has no duplicate patient IDs, has an equal number of males and females so I used a custom even smaller dataset to show the functionality of DicomSplit and DataSplit below . Viewing the Data . The show function is specifically tailored to display .dcm formats. By customizing the show function we have now view patient information with each image . @patch @delegates(show_image) def show_dinfo(self:DcmDataset, scale=True, cmap=plt.cm.bone, min_px=-1100, max_px=None, **kwargs): &quot;&quot;&quot;show function that prints patient attributes from DICOM head&quot;&quot;&quot; px = (self.windowed(*scale) if isinstance(scale,tuple) else self.hist_scaled(min_px=min_px,max_px=max_px,brks=scale) if isinstance(scale,(ndarray,Tensor)) else self.hist_scaled(min_px=min_px,max_px=max_px) if scale else self.scaled_px) print(f&#39;Patient Age: {self.PatientAge}&#39;) print(f&#39;Patient Sex: {self.PatientSex}&#39;) print(f&#39;Body Part Examined: {self.BodyPartExamined}&#39;) print(f&#39;Rows: {self.Rows} Columns: {self.Columns}&#39;) show_image(px, cmap=cmap, **kwargs) . patient = 7 sample = dcmread(items[patient]) sample.show_dinfo() . Patient Age: 31 Patient Sex: M Body Part Examined: CHEST Rows: 1024 Columns: 1024 . DICOM formats contain alot of useful information but difficult to see image by image so we need to capture this information and create a dataframe for better viewing and data manipulation. . Create a dataframe Customize the functions so that we include what we want in our dataframe . #updating to accomodate def _dcm2dict2(fn, **kwargs): t = fn.dcmread() return fn, t.PatientID, t.PatientAge, t.PatientSex, t.BodyPartExamined, t.Modality, t.Rows, t.Columns, t.BitsStored, t.PixelRepresentation @delegates(parallel) def _from_dicoms2(cls, fns, n_workers=0, **kwargs): return pd.DataFrame(parallel(_dcm2dict2, fns, n_workers=n_workers, **kwargs)) pd.DataFrame.from_dicoms2 = classmethod(_from_dicoms2) . test_df = pd.DataFrame.from_dicoms2(items) test_df.columns=[&#39;file&#39;, &#39;PatientID&#39;, &#39;Age&#39;, &#39;Sex&#39;, &#39;Bodypart&#39;, &#39;Modality&#39;, &#39;Rows&#39;, &#39;Cols&#39;, &#39;BitsStored&#39;, &#39;PixelRep&#39; ] test_df.to_csv(&#39;test_df.csv&#39;) test_df.head() . file PatientID Age Sex Bodypart Modality Rows Cols BitsStored PixelRep . 0 C: Users avird .fastai data siim_small sm No Pneumothorax 000000 - Copy.dcm | 16d7f894-55d7-4d95-8957-d18987f0e981 | 62 | M | CHEST | CR | 1024 | 1024 | 8 | 0 | . 1 C: Users avird .fastai data siim_small sm No Pneumothorax 000000.dcm | 16d7f894-55d7-4d95-8957-d18987f0e981 | 62 | M | CHEST | CR | 1024 | 1024 | 8 | 0 | . 2 C: Users avird .fastai data siim_small sm No Pneumothorax 000002.dcm | 850ddeb3-73ac-45e0-96bf-7d275bc83782 | 52 | F | CHEST | CR | 1024 | 1024 | 8 | 0 | . 3 C: Users avird .fastai data siim_small sm No Pneumothorax 000005.dcm | e0fd6161-2b8d-4757-96bc-6cf620a993d5 | 65 | F | CHEST | CR | 1024 | 1024 | 8 | 0 | . 4 C: Users avird .fastai data siim_small sm No Pneumothorax 000006 - Copy.dcm | 99171908-3665-48e8-82c8-66d0098ce209 | 52 | F | CHEST | CR | 1024 | 1024 | 8 | 0 | . We can now view the information (note this for my custom dataset) . #Plot 3 comparisons def plot_comparison(df, feature, feature1, feature2): &quot;Plot 3 comparisons from a dataframe&quot; fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (16, 4)) s1 = sns.countplot(df[feature], ax=ax1) s1.set_title(feature) s2 = sns.countplot(df[feature1], ax=ax2) s2.set_title(feature1) s3 = sns.countplot(df[feature2], ax=ax3) s3.set_title(feature2) plt.show() . plot_comparison(test_df, &#39;PatientID&#39;, &#39;Sex&#39;, &#39;Bodypart&#39;) . Age comparison . def age_comparison(df, feature): &quot;Plot hisogram of age range in dataset&quot; fig, (ax1) = plt.subplots(1,1, figsize = (16, 4)) s1 = sns.countplot(df[feature], ax=ax1) s1.set_title(feature) plt.show() age_comparison(test_df, &#39;Age&#39;) . Modelling . Considerations: . patient overlap between train and val set | sampling - how many negative and postive cases are in the train/val split (class imbalance) | augmentations - consideration of what augmentations are used and why in some cases may not be useful | . Patient Overlap . It is important to know if there is going to be any patient overlap when creating the train and validation sets as this may lead to an overly optimistic test set. The great thing about DICOMs is that we can check to see if there are any duplicate patientIDs in the test and valid sets when we split our data . def DicomSplit(valid_pct=0.2, seed=None, **kwargs): &quot;Splits `items` between train/val with `valid_pct`&quot; &quot;and checks if identical patient IDs exist in both the train and valid sets&quot; def _inner(o, **kwargs): train_list=[]; valid_list=[] if seed is not None: torch.manual_seed(seed) rand_idx = L(int(i) for i in torch.randperm(len(o))) cut = int(valid_pct * len(o)) trn = rand_idx[cut:]; trn_p = o[rand_idx[cut:]] val = rand_idx[:cut]; val_p = o[rand_idx[:cut]] for i, im in enumerate(trn_p): trn = im.dcmread() patient_ID = trn.PatientID train_list.append(patient_ID) for j, jm in enumerate(val_p): val = jm.dcmread() vpatient_ID = val.PatientID valid_list.append(vpatient_ID) print(set(train_list) &amp; set(valid_list)) return rand_idx[cut:], rand_idx[:cut] return _inner . set_seed(7) trn,val = DicomSplit(valid_pct=0.2)(items) trn, val . {&#39;6224213b-a185-4821-8490-c9cba260a959&#39;} . ((#21) [2,13,9,12,11,24,8,14,16,6...], (#5) [19,18,3,23,17]) . The custom test dataset only has 26 images which is split in a test set of 24 and a valid set of 5 using valid_pct of 0.2. By customizing RandomSplitter into DicomSplit you can view to see if there are any duplicate PatientIDs. In this case there is a duplicate ID: 6224213b-a185-4821-8490-c9cba260a959 . Using set_seed allows for reproducible results and ensures we use the same seed when training . Sampling . This dataset has 2 classes Pneumothorax and No Pneumothorax, DataSplit looks at how many Pneumothorax and No Pneumothorax images are in the train and valid sets. This is to view how fair the train/val split is to ensure good model sampling . def DataSplit(valid_pct=0.2, seed=None, **kwargs): &quot;Check the number of each class in train and valid sets&quot; def _inner(o, **kwargs): train_list=[]; valid_list=[] if seed is not None: torch.manual_seed(seed) rand_idx = L(int(i) for i in torch.randperm(len(o))) cut = int(valid_pct * len(o)) trn_p = o[rand_idx[cut:]] val_p = o[rand_idx[:cut]] for p in enumerate(trn_p): b = str(p).split(&#39;/&#39;)[7] train_list.append(b) for q in enumerate(val_p): e = str(q).split(&#39;/&#39;)[7] valid_list.append(e) print(f&#39;train: {train_list} n valid: {valid_list}&#39;) return rand_idx[cut:], rand_idx[:cut] return _inner . using the same set_seed we can get reproducible results . set_seed(7) trn,val = DataSplit(valid_pct=0.2)(items) trn, val . train: [&#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;] valid: [&#39;Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;] . ((#21) [2,13,9,12,11,24,8,14,16,6...], (#5) [19,18,3,23,17]) . With this test dataset (the train set has 14 No Pneumothorax and 5 Pneumothorax images and the valid set has 4 No Pneumothorax and 3 Pneumothorax images . &gt;&gt;Work in progress work on various techniques to help with unbalanced datasets especially true for medical image datasets where there are typically alot more images of &#39;normal&#39; compared to &#39;diseased&#39; (oversampling, k-fold) . Augmentations . img1 = (pneumothorax_source/&#39;chest1.png&#39;); img2 = (pneumothorax_source/&#39;chest2.png&#39;) . &gt;&gt;Work in progress Choosing the right augmentations is important in determing how it affects the sampling process. For example in some cases it may not be a good idea to flip images. . Here is an image of a &#39;normal&#39; patient in its correct orientation (heart showing in the middle right) . Image.open(img1) . If we flip the image . Image.open(img2) . We can now see the heart is middle left. If the classifier was looking to detect defects of the heart then this type of augmentation would not be suitable. . #clear out some memory import gc gc.collect() . 33619 . #switch back to the full dataset items = get_dicom_files(pneumothorax_source, recurse=True, folders=&#39;train&#39;) df = pd.read_csv(pneumothorax_source/f&quot;labels.csv&quot;) . xtra_tfms = [RandomResizedCrop(224)] batch_tfms = [*aug_transforms(do_flip=False, flip_vert=False, xtra_tfms=xtra_tfms), Normalize.from_stats(*imagenet_stats)] . set_seed(7) pneumothorax = DataBlock(blocks=(ImageBlock(cls=PILDicom), CategoryBlock), get_x=lambda x:pneumothorax_source/f&quot;{x[0]}&quot;, get_y=lambda x:x[1], splitter=RandomSplitter(valid_pct=0.3), item_tfms=Resize(256), batch_tfms=batch_tfms) dls = pneumothorax.dataloaders(df.values, bs=12, num_workers=0) . Check train and valid sizes . len(dls.train_ds), len(dls.valid_ds) . (175, 75) . precision = Precision() recall = Recall() . net = xresnext18(pretrained=False, sa=True, act_cls=Mish, n_out=dls.c) learn = Learner(dls, net, metrics=[accuracy, precision, recall], cbs=[ShowGraphCallback(), CollectDataCallback]) . If you do not specifiy a loss function or optimization function fastai automatically allocates one. In this example using defaults . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . learn.opt_func . &lt;function fastai2.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)&gt; . learn.lr_find() . SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.0014454397605732083) . learn.freeze() learn.fit_one_cycle(1, slice(3e-3)) . epoch train_loss valid_loss accuracy precision_score recall_score time . 0 | 0.715601 | 0.857002 | 0.666667 | 0.307692 | 0.200000 | 00:08 | . learn.unfreeze() learn.fit_one_cycle(5, slice(3e-3)) . epoch train_loss valid_loss accuracy precision_score recall_score time . 0 | 0.671033 | 0.764320 | 0.493333 | 0.304348 | 0.700000 | 00:08 | . 1 | 0.674126 | 1.674710 | 0.293333 | 0.260870 | 0.900000 | 00:08 | . 2 | 0.664581 | 0.742959 | 0.733333 | 0.000000 | 0.000000 | 00:08 | . 3 | 0.633107 | 0.670259 | 0.640000 | 0.315789 | 0.300000 | 00:08 | . 4 | 0.617154 | 0.619417 | 0.693333 | 0.333333 | 0.150000 | 00:08 | . C: Users avird Anaconda3 envs fastmed lib site-packages sklearn metrics _classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . def show_results2(self, ds_idx=1, dl=None, max_n=9, shuffle=False, **kwargs): if dl is None: dl = self.dls[ds_idx].new(shuffle=shuffle) b = dl.one_batch() t,a,preds = self.get_preds(dl=[b], with_decoded=True) print(f&#39;Acutal: {a} n Preds: {preds} n&#39;) self.dls.show_results(b, preds, max_n=max_n, **kwargs) . show_results2(learn, max_n=12, nrows=2, ncols=6) . Acutal: TensorCategory([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1], dtype=torch.int32) Preds: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]) . Model Evaluation . Because medical models are high impact it is important to know how good a model is at detecting a certain condition. . The above model has an accuracy of 69%. One needs to look deeper into how the accuracy of 69% was calculated and whether it is acceptable. . Accuracy is the probablity that the model is correct or | . Accuracy is the probability that the model is correct and the patient has the condition PLUS the probability that the model is correct and the patient does not have the condition | . There are some other key terms that need to be used when evaluating medical models: . False Negative is an error in which a test result improperly indicates no presence of a condition (the result is negative), when in reality it is present. | . False Positive is an error in which a test result improperly indicates presence of a condition, such as a disease (the result is positive), when in reality it is not present | . Sensitivity or True Positive Rate is where the model classifies a patient has the disease given the patient actually does have the disease. Sensitivity quantifies the avoidance of false negatives | . Example: A new test was tested on 10,000 patients, if the new test has a sensitivity of 90% the test will correctly detect 9,000 (True Positive) patients but will miss 1000 (False Negative) patients that have the condition but were tested as not having the condition . Specificity or True Negative Rate is where the model classifies a patient as not having the disease given the patient actually does not have the disease. Specificity quantifies the avoidance of false positives | . Understanding and using sensitivity, specificity and predictive values is a great paper if you are interested in learning more . PPV and NPV . Most medical testing is evaluated via PPV (Postive Predictive Value) or NPV (Negative Predictive Value). . PPV - if the model predicts a patient has a condition what is probabilty that the patient actually has the condition . | NPV - if the model predicts a patient does not have a condition what is the probability that the patient actually does not have the condition . | . The ideal value of the PPV, with a perfect test, is 1 (100%), and the worst possible value would be zero . The ideal value of the NPV, with a perfect test, is 1 (100%), and the worst possible value would be zero . Plot a confusion matrix - note that this is plotted against the valid dataset size which is 75 in this case . interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() len(dls.valid_ds)==len(losses)==len(idxs) interp.plot_confusion_matrix(figsize=(7,7)) . upp, low = interp.confusion_matrix() tn, fp = upp[0], upp[1] fn, tp = low[0], low[1] print(tn, fp, fn, tp) . 49 6 17 3 . Sensitivity = True Positive/(True Positive + False Negative) . sensitivity = tp/(tp + fn) sensitivity . 0.15 . In this case the model only has a sensitivity of 15% and hence is only capable of correctly detecting 15% True Positives(ie who have Pneumothorax) but will miss 85% of False Negatives (patients that actually have Pneumothorax but were told they did not! Not a good situation to be in). . This is also know as a Type II error . Specificity = True Negative/(False Positive + True Negative) . specificity = tn/(fp + tn) specificity . 0.8909090909090909 . In this case the model has a specificity of 89% and hence can correctly detect 89% of the time that a patient does not have Pneumothorax but will incorrectly classify that 11% of the patients have Pneumothorax (False Postive) but actually do not. . This is also known as a Type I error . Positive Predictive Value (PPV) . ppv = tp/(tp+fp) ppv . 0.3333333333333333 . In this case the model performs poorly in correctly predicting patients with Pneumothorax . Negative Predictive Value (NPV) . npv = tn/(tn+fn) npv . 0.7424242424242424 . This model is better at predicting patients with No Pneumothorax . Some of these metrics can be calculated using sklearn&#39;s classification report . interp.print_classification_report() . precision recall f1-score support No Pneumothorax 0.74 0.89 0.81 55 Pneumothorax 0.33 0.15 0.21 20 accuracy 0.69 75 macro avg 0.54 0.52 0.51 75 weighted avg 0.63 0.69 0.65 75 . Calculating Accuracy . The accuracy of this model as mentioned before is 69% - lets now calculate this! . We can also look at Accuracy as: . accuracy = sensitivity prevalence + specificity (1 - prevalence) . Prevalence is a statistical concept referring to the number of cases of a disease that are present in a particular population at a given time. . The prevalence in this case is how many patients in the valid dataset have the condition compared to the total number. To view the number of Pneuomothorax patients in the valid set . t= dls.valid_ds.cat #t[0] . There are 20 Pneumothorax images in the valid set hence the prevalance here is 20/75 = 0.27 . accuracy = (0.15 * 0.27) + (0.89 * (1 - 0.27)) accuracy . 0.6901999999999999 . The question is, is this model ready for medical use? . fin .",
            "url": "https://asvcode.github.io/Blogs/medical_imaging/dicom/model_evaluation/ppv/npv/specificity/sensitivity/2020/04/29/Medical-Imaging-Starter.html",
            "relUrl": "/medical_imaging/dicom/model_evaluation/ppv/npv/specificity/sensitivity/2020/04/29/Medical-Imaging-Starter.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastai2 Image Augmentation",
            "content": "Intro . Fastai2 provides a wide range of data augmentation techniques and this blog is particularly focused on image augmentation techniques (This is a update to the article &#39;Data Augmentation Techniques&#39; I wrote in 2018 using fastai v1[1]) . Working with limited data has its own challenges, using data augmentation can have positive results only if the augmentation techniques enhance the current data set for example is there any worth is training a network to ‘learn’ about a landmark in a flipped upside down orientation? . Invariance is the ability of convolutional neural networks to classify objects even when they are placed in different orientations. Data augmentation is a way of creating new ‘data’ with different orientations. The benefits of this are two fold, the first being the ability to generate ‘more data’ from limited data and secondly it prevents over fitting. . Most deep learning libraries use a step by step method of augmentation whilst *fastai2 utilizes methods that combine various augmentation parameters to reduce the number of computations and reduce the number of lossy operations*[2]. . Fastai uses Pipelines to compose several transforms together. A Pipeline is defined by passing a list of Transforms and it will then compose the transforms inside it. In this blog I will look at what order these transforms are conducted and what effect they have on image quality and efficiency. Pipelines are sorted by the internal order atribute (more discussed below) with a default order of 0. . Using this as a high-level API example . from fastai2.vision.all import* source = untar_data(URLs.PETS) . #High-level API example testblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=[Resize(256)], batch_tfms=[*aug_transforms(xtra_tfms=None)]) test_dls = testblock.dataloaders(source/&#39;images&#39;) . test_dls.show_batch(max_n=6, nrows=1, ncols=6) . To check the order of how augementations are conducted we can call *after_item* and *after_batch* . after_item . test_dls.after_item . Pipeline: Resize -&gt; ToTensor . In this case images are: . resized to sizes of equal length, in this case 256 and then . convert the image into a *channel* X *height* X *weigth* tensor . But what does Resize do? . Click the button to view the Resize class . #collapse #https://github.com/fastai/fastai2/blob/master/nbs/09_vision.augment.ipynb @delegates() class Resize(RandTransform): split_idx = None mode,mode_mask,order,final_size = Image.BILINEAR,Image.NEAREST,1,None &quot;Resize image to `size` using `method`&quot; def __init__(self, size, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection, resamples=(Image.BILINEAR, Image.NEAREST), **kwargs): super().__init__(**kwargs) self.size,self.pad_mode,self.method = _process_sz(size),pad_mode,method self.mode,self.mode_mask = resamples def before_call(self, b, split_idx): if self.method==ResizeMethod.Squish: return self.pcts = (0.5,0.5) if split_idx else (random.random(),random.random()) def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)): orig_sz = _get_sz(x) self.final_size = self.size if self.method==ResizeMethod.Squish: return x.crop_pad(orig_sz, Tuple(0,0), orig_sz=orig_sz, pad_mode=self.pad_mode, resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size) w,h = orig_sz op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad] m = w/self.size[0] if op(w/self.size[0],h/self.size[1]) else h/self.size[1] cp_sz = (int(m*self.size[0]),int(m*self.size[1])) tl = Tuple(int(self.pcts[0]*(w-cp_sz[0])), int(self.pcts[1]*(h-cp_sz[1]))) return x.crop_pad(cp_sz, tl, orig_sz=orig_sz, pad_mode=self.pad_mode, resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size) . . By default resize &#39;squishes&#39; the image to the size specified. The image is resized so that the shorter dimension matches the size specifed and the rest padded with what is specified in pad_mode. . The method parameter can be be 1 of 3 values: Crop, Pad or Squish(default) eg: *method=ResizeMethod.Squish* The padding parameter also takes 1 of 3 values: Border, Zeros and Reflection(default) eg: *pad_mode=PadMode.Reflection*. . The images are resized/resamples using bilinear and nearest neighbour interprolations[3]. . We can check to see how initial image sizes are affected by Resize. I choose an image with numbers so that you can see different areas of the image easier and I colored each of the corners a different color to better see what effects Resize has on the image. . #Load a test image image_path = &#39;C:/Users/avird/.fastai/data/0100-number_12.jpg&#39; img = Image.open(image_path) img.shape, type(img) . ((380, 500), PIL.JpegImagePlugin.JpegImageFile) . #Convert image into a fastai.PILImage img = PILImage(PILImage.create(image_path).resize((500,380))) img.shape, type(img) . ((380, 500), fastai2.vision.core.PILImage) . #View the image img . Fastai uses 3 types of resize methods (using *ResizeMethod*: Squish, Pad and Crop) and they can be plotted against each other to view the differences between them. Squish is the fastai default. To better view the differences I used a padding of zeros.(the default for padding is Reflection) . &gt; Image size 5 . #collapse #Use image size of 5 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(5, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=1), ctx=ax, title=f&#39;{method}, size=5&#39;); . . Using an image size of 5 we can see how the image is affected by Resize. At this size we can see all the 4 different colors in each corner and there is not much difference between squish abd crop. With pad however the image is being resized so the shorter dimension (in this case the height (as the original image size is 380 height and 500 width) is matched to the image size of 256 and then padded with zeros. . &gt; Image size 15 . #collapse #Use image size of 15 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(15, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=1), ctx=ax, title=f&#39;{method}, size=15&#39;); . . At image size 15, both &#39;squish&#39; and &#39;pad&#39; are still showing all the colors in the corners but with &#39;crop&#39; you start to notice that the colors in each corner are begining to fade as the image is being cropped from the center . &gt; Image size 256 . #collapse #Use image size of 256 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(256, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=0), ctx=ax, title=f&#39;{method}, size=256&#39;); . . At 256 both &#39;squish&#39; and &#39;pad&#39; still display the full image and &#39;crop&#39; displays the cropped image . What impacts could this have on real datasets. . Using an image from a Covid19 dataset [5] . #collapse test_path = &#39;C:/Users/avird/.fastai/data/0002.jpeg&#39; testimg = Image.open(test_path) img2 = PILImage(PILImage.create(test_path).resize((944, 656))) img2 . . #collapse #Use image size of 256 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(256, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img2, split_idx=1), ctx=ax, title=f&#39;{method}, size=256&#39;); . . In this case: . the default &#39;squish&#39; resize method squishes the image on the horizontal axis. You can view the whole image however you can see that ribcage has been constricted towards the center. The implications of this could mean that important features you see in the original image could either be erased or diluted. . for the &#39;pad&#39; resize the image is still viewable fully but again the image has been squished on the vertical axis. . With &#39;crop&#39;, the image is cropped from the centre hence we lose image details from the edges . The implications of these choices is really dependant on the dataset but they could have an detrimental effect if the wrong choice is choosen leading to vital features being erased or diluted . after_batch . Back to the pets example if we run *after_batch*, this shows us the after batch augmentation pipeline. Previously item_tfms is used to resize the images and to collate them into tensors ready for GPU processing. . test_dls.after_batch . Pipeline: IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm . This reveals the pipeline process for the batch transformations: . convert ints to float tensors . apply all the affine transformations . followed by the lighting transformations . The order is important in order to maintain a number of key aspects:&gt; Maintain image quality . Reduce computations . Improve efficiency . As mentioned in Fastbook [4], most machine libraries use a step by step process of augmentation which can lead to a reduced quality of images. The datablock example above is an example of a high-level API which is pretty flexible but not as much as a mid-level API. . The mid-level datablock below is an exact example of the high-level datablock above and allows for more customizations and we will use this datablock for the rest of the blog . #collapse #Helper for viewing single images def repeat_one(source, n=128): &quot;&quot;&quot;Single image helper for displaying batch&quot;&quot;&quot; return [get_image_files(source)[1]]*n . . Mid-Level API and viewing a batch of a single image . #mid-level API example #num_workers = 0 because I use windows :) and windows does not support multiprocessing on CUDA [6] tfms = [[PILImage.create], [using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), Categorize]] item_tfms = [ToTensor(), Resize(256)] splitter=RandomSplitter(seed=42) after_b = [IntToFloatTensor(), *aug_transforms(xtra_tfms=RandomResizedCrop(256), min_scale=0.9)] dsets = Datasets(repeat_one(source/&#39;images&#39;), tfms=tfms) dls = dsets.dataloaders(after_item=item_tfms, after_batch=after_b, bs=32, num_workers=0, splits=splitter) . dls.after_batch . Pipeline: RandomResizedCrop -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm . #collapse dls.show_batch(max_n=6, nrows=1, ncols=6) . . Image comparisons (Fastai v The Rest) . #create 1 batch x,y = dls.one_batch() . Checking image quality and speed using step by step transformations. . #collapse time x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=256) x1 = x1.brightness(max_lighting=0.2, p=1.) x1 = x1.zoom(max_zoom=1.1, p=0.5) x1 = x1.warp(magnitude=0.2, p=0.5) _,axs = subplots(1, 1, figsize=(5,5)) TensorImage(x1[0]).show(ctx=axs[0]) . . Wall time: 103 ms . &lt;matplotlib.axes._subplots.AxesSubplot at 0x29421ecdfc8&gt; . Checking image quality and speed using fastai2 . #collapse %%time tfms = setup_aug_tfms([Brightness(max_lighting=0.2, p=1.,), CropPad(size=256), Zoom(max_zoom=1.1, p=0.5), Warp(magnitude=0.2, p=0.5) ]) x = Pipeline(tfms)(x) _,axs = subplots(1, 1, figsize=(5,5)) TensorImage(x[0]).show(ctx=axs[0]) . . Wall time: 45.9 ms . &lt;matplotlib.axes._subplots.AxesSubplot at 0x294220787c8&gt; . Comparing the times above using a pipeline where a list of transforms are passed in is nearly twice as fast as using augmentations step by step. In this case the step by step method completed the task in 103ms compard to 46s using fastai . Looking at side by side look at image quality . #collapse image_comp(): x,y = dls.one_batch() tfms = setup_aug_tfms([Brightness(max_lighting=0.3, p=1.,), Resize(size=256), Zoom(max_zoom=1.1, p=1.), Warp(magnitude=0.2, p=1.) ]) x = Pipeline(tfms)(x) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=256) x1 = x1.brightness(max_lighting=0.3, p=1.) x1 = x1.zoom(max_zoom=1.1, p=1.) x1 = x1.warp(magnitude=0.2, p=1.) _,axs = subplots(1, 2, figsize=(20,20)) TensorImage(x[0]).show(ctx=axs[0], title=&#39;fastai&#39;) TensorImage(x1[0]).show(ctx=axs[1], title=&#39;other&#39;) . . image_comp() . You can definately see differences between the two pictures, the &#39;fastai&#39; image is more clearer compared to the &#39;other&#39; image. How about some other examples . image_comp() . image_comp() . List of Transforms . There a number of transforms and here is a list of the most common ones . RandomResizedCrop = &quot;Picks a random scaled crop of an image and resize it to size - order 1&quot; IntToFloatTensor = &quot;Transform image to float tensor, optionally dividing by 255 (e.g. for images) - order 10 Rotate = &quot;Apply a random rotation of at most max_deg with probability p to a batch of images&quot; Brightness = &quot;Apply change in brightness of max_lighting to batch of images with probability p.&quot; RandomErasing = &quot;Randomly selects a rectangle region in an image and randomizes its pixels.&quot; - order 100 CropPad = &quot;Center crop or pad an image to size&quot; - order 0 Zoom = &quot;Apply a random zoom of at most max_zoom with probability p to a batch of images&quot; Warp = &quot;Apply perspective warping with magnitude and p on a batch of matrices&quot; Contrast = &quot;Apply change in contrast of max_lighting to batch of images with probability p.&quot; . Pipeline for multiple augmentations . In the example above the after_batch pipeline consisted of IntToFloatTensor &gt; Affine tranformations &gt; Lighting transformations. . However what we uses additional augmentations, what does the pipeline look like then? . #collapse source = untar_data(URLs.PETS) #Helper for viewing single images def repeat_one(source, n=128): &quot;&quot;&quot;Single image helper for displaying batch&quot;&quot;&quot; return [get_image_files(source)[2]]*n . . #collapse #Include multiple transforms tfms = [[PILImage.create], [using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), Categorize]] item_tfms = [ToTensor(), Resize(296)] splitter=RandomSplitter(seed=42) xtra_tfms = [Rotate(max_deg=45, p=1.), RandomErasing(p=1., max_count=10, min_aspect=0.5, sl=0.2, sh=0.2), RandomResizedCrop(p=1., size=256), Brightness(max_lighting=0.2, p=1.), CropPad(size=256), Zoom(max_zoom=2.1, p=0.5), Warp(magnitude=0.2, p=1.0) ] after_b = [IntToFloatTensor(), *aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=0., max_zoom=1.1, max_lighting=0.,max_warp=0., p_affine=0.75, p_lighting=0.75, xtra_tfms=xtra_tfms, size=256, mode=&#39;bilinear&#39;, pad_mode=PadMode.Reflection, align_corners=True, batch=False, min_scale=0.9)] mdsets = Datasets(repeat_one(source/&#39;images&#39;), tfms=tfms) mdls = mdsets.dataloaders(after_item=item_tfms, after_batch=after_b, bs=32, num_workers=0, splits=splitter) . . Looking at after_item - it is the same as before . mdls.after_item . Pipeline: Resize -&gt; ToTensor . mdls.after_batch . Pipeline: CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm -&gt; RandomErasing . after_batch is now a different story and we can see the list of how fastai computes its augmentations. These are all done in sequence (depending on their order) starting with . CropPad . followed by affine . lighting . and random erasing transforms. . Here is what the batch looks like . mdls.show_batch(max_n=6, nrows=1, ncols=6) . The order number determines the sequence of the transforms for example CropPad is order 0, Resize and RandomCrop are order 1 hence the reason they appear first on the list. IntToFloatTensor is order 10 and runs after PIL transforms on the GPU. Affine transforms are order 30 and so is RandomResizedCropGPU and lighting transforms are order 40. RandomErasing is order 100. . Viewing the order of transforms . #for example mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . RandomResizedCrop.order, CropPad.order, IntToFloatTensor.order, AffineCoordTfm.order, RandomResizedCropGPU.order, RandomErasing.order . (0, 0, 10, 30, 30, 100) . You can force the order by implicity specifying the order of a transform by stating the order within a transform class. . Interesting Observations . There were some interesting observations during this experimention. Adding a *min_scale* value in aug_transforms adds RandomResizedCropGPU to the pipeline . mdls.after_batch . Pipeline: CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . However if you add RandomResizedCrop as well as a min_scale value the pipeline now looks like this . mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . And if you use RandomResizedCrop with no min_scale value the pipeline is now: . mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm -&gt; RandomErasing . Still to do . There is clearly a plethora of options and additonal experimentation is needed to see what the impact of the various pipelines are on image quality, efficiency and end results -*work in progress* . Manually going through the pipeline . Attempt to manually go throught the pipeline. . #collapse image_path = &#39;C:/Users/avird/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39; TEST_IMAGE = Image.open(image_path) img = PILImage(PILImage.create(image_path)) img.shape, type(img) . . ((400, 600), fastai2.vision.core.PILImage) . This is the original image of size 400 height and 600 width . #collapse #Original Image img . . Resize to 256 using default crop and reflection padding . #collapse #Resize to 256 using default crop and reflection padding r = Resize(256, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection) w = r(img) w.shape, type(w) . . ((256, 256), fastai2.vision.core.PILImage) . #collapse w . . Crop the image using size 256 . #collapse Crop crp = CropPad(256) c = r(crp(img)) c.shape, type(c) . . ((256, 256), fastai2.vision.core.PILImage) . #collapse c . . Convert PILImage into a TensorImage . timg = TensorImage(array(c)).permute(2,0,1).float()/255. timg.shape, type(timg) . (torch.Size([3, 256, 256]), fastai2.torch_core.TensorImage) . h = TensorImage(timg[None].expand(3, *timg.shape).clone()) h.shape, type(h) . (torch.Size([3, 3, 256, 256]), fastai2.torch_core.TensorImage) . #if do_flip=true and flip-vert=false = Flip fli = Flip(p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y = fli(h) show_image(y[0], ctx=ax, cmap=&#39;Greys&#39;) . #if do_flp=true and flip_vert=true = dihyderal dih = Dihedral(p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y1 = dih(h) show_image(y1[0], ctx=ax) . #Rotate rot = Rotate(max_deg=45, p=1.) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y2 = rot(h) show_image(y2[0], ctx=ax) . # Zoom zoo = Zoom(max_zoom=4.1, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y3 = zoo(h) show_image(y3[0], ctx=ax) . # Warp war = Warp(magnitude=0.7, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y4 = war(h) show_image(y4[0], ctx=ax) . #Brightness bri = h.brightness(draw=0.9, p=1.) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y5 = bri show_image(y5[0], ctx=ax) . #Contrast con = h.contrast(draw=1.9, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y6 = con show_image(y6[0], ctx=ax) . View the images side by side . #collapse _,axs = plt.subplots(1,8, figsize=(20,9)) for i,ax in enumerate(axs.flatten()): y7 = y1 + y4 show_image(img, ctx=axs[0], title=&#39;original&#39;) show_image(w, ctx=axs[1], title=&#39;resize 256&#39;) show_image(y[0], ctx=axs[2], title=&#39;flip&#39;) show_image(y2[0], ctx=axs[3], title=&#39;rotate&#39;) show_image(y3[0], ctx=axs[4], title=&#39;zoom&#39;) show_image(y4[0], ctx=axs[5], title=&#39;warp&#39;) show_image(y5[0], ctx=axs[6], title=&#39;brighness&#39;) show_image(y6[0], ctx=axs[7], title=&#39;contrast&#39;) . . References: . 1: https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b . 2: https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb . 3: https://github.com/fastai/fastai2/blob/master/nbs/09_vision.augment.ipyn . 4: https://github.com/fastai/fastbook . 5: https://github.com/lindawangg/COVID-Net . 6: https://forums.fast.ai/t/windows-runtimeerror-cuda-runtime-error-801-runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type/57333 .",
            "url": "https://asvcode.github.io/Blogs/fastai/augmentation/image-augmentation/2020/03/26/Fastai2-Image-Augmentation.html",
            "relUrl": "/fastai/augmentation/image-augmentation/2020/03/26/Fastai2-Image-Augmentation.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Covid 19 Jamii Yako Na Wewe Mtazamo Wa Sayansi Ya Data",
            "content": "Covid-19, jamii yako, na wewe - mtazamo wa sayansi ya data . This is the Swahili translation of the article written by Jeremy Howard and Rachel Thomas Covid-19, your community, and you — a data science perspective . Imeandikwa: 09 Mar 2020 by Jeremy Howard and Rachel Thomas . Ilitafsiriwa na Amrit Virdee . Sisi ni wanasayansi wa data-Hiyo ni, kazi yetu ni kuelewa jinsi ya kuchambua na kutafsiri data. Wakati tunachambua data karibu covid-19, tunajali sana. Sehemu zilizo hatarini zaidi za jamii, wazee na masikini, ziko hatarini zaidi, lakini kudhibiti kuenea na athari za ugonjwa kunahitaji sisi sote kubadili tabia yetu. Osha mikono yako vizuri na mara kwa mara, epuka vikundi na umati wa watu, futa matukio, na usiguse uso wako. Katika chapisho hili, tunaelezea kwa nini tunajali, na unapaswa kuwa pia. Kwa muhtasari bora wa habari muhimu unayohitaji kujua, soma Corona kwa kifupi na Ethan Alley (rais wa mashirika yasiyo ya faida ambayo huendeleza teknolojia za kupunguza hatari kutoka kwa magonjwa ya milipuko). . Tafsiri . Mtu yeyote anakaribishwa kutafsiri kifungu hiki, kusaidia jamii zao za mitaa kuelewa maswala haya. Tafadhali unganisha hapa na mkopo unaofaa. Tujulishe kwenye Twitter ili tuweze kuongeza tafsiri yako kwenye orodha hii. . French . Spanish . German . Portuguese (Brazil) . Chinese 中文简体 . Thai . Tunahitaji mfumo wa matibabu unaofanya kazi . Zaidi ya miaka 2 iliyopita mmoja wetu (Rachel) alipata maambukizi ya ubongo ambayo huua karibu 1/4 ya watu wanaopata, na huacha 1/3 akiwa na udhaifu wa kudumu wa utambuzi. Wengine wengi huishia na maono ya kudumu na uharibifu wa kusikia. Rachel alikuwa mwepesi wakati wa kutambaa katika maegesho ya hospitali. Alikuwa na bahati ya kutosha kupata utunzaji wa haraka, utambuzi, na matibabu. Hadi muda mfupi kabla ya tukio hili Rachel alikuwa kwenye afya njema. Kuwa na ufikiaji wa haraka katika chumba cha dharura hakika kuliokoa maisha yake. . Sasa, wacha tuzungumze juu ya covid-19, na nini kinaweza kutokea kwa watu walio katika hali ya Rachel katika wiki na miezi ijayo. Idadi ya watu waliopatikana wameambukizwa covid-19 mara mbili kila siku 3 hadi 6. Pamoja na kiwango cha kuongezeka kwa siku tatu, hiyo inamaanisha kwamba idadi ya watu waliopatikana wameambukizwa inaweza kuongezeka mara 100 katika wiki tatu (sio rahisi sana, lakini wasikatishwe na maelezo ya kiufundi). Mtu mmoja kati ya 10 aliyeambukizwa anahitaji kulazwa hospitalini kwa wiki nyingi, na nyingi hizi zinahitaji oksijeni. Ingawa ni siku za mapema sana kwa virusi hivi, tayari kuna mikoa ambayo hospitali zimezidiwa kabisa, na watu hawana uwezo wa kupata matibabu wanayohitaji (sio tu kwa covid-19, bali pia kwa kitu kingine chochote, kama vile huduma ya kuokoa maisha ambayo Rachel alihitaji). Kwa mfano, nchini Italia, ambapo wiki moja tu maafisa walikuwa wakisema kwamba kila kitu kiko sawa, sasa watu milioni kumi na sita wamewekwa chini (sasisha: masaa 6 baada ya kuchapisha hii, Italia iliiweka nchi nzima chini, na mahema kama haya yanawekwa kusaidia kushughulikia kuongezeka kwa wagonjwa: . Hema la matibabu linalotumika huko Italia . . Dr Antonio Pesenti, mkuu wa kitengo cha kukabiliana na msiba wa mkoa katika eneo lililo ngumu sana nchini Italia, alisema, “Sasa tunalazimishwa kuanzisha matibabu ya matunzo makubwa katika barabara, katika uwanja wa michezo, katika vyumba vya uokoaji … Moja ya mifumo bora ya afya ulimwenguni, huko Lombardy ni hatua mbali na kuanguka. “. . Hii sio kama mafua . Homa hiyo ina kiwango cha vifo takriban 0.1% ya maambukizo. Marc Lipsitch, mkurugenzi wa Kituo cha Dalili za Magonjwa ya Kuambukiza huko Harvard, anakadiria kuwa kwa covid-19 ni 1-2%. Mitindo ya hivi karibuni ya epedemiological ilipata kiwango cha 1.6% nchini Uchina mnamo Februari, mara kumi na sita zaidi kuliko flu[1] (hii inaweza kuwa nambari ya kihafidhina, kwa sababu viwango vinapanda sana wakati mfumo wa matibabu hauwezi kuhimili). Makadirio ya sasa bora yanatarajia kwamba covid-19 ataua watu zaidi ya mara 10 mwaka huu kuliko homa hiyo (na kuigwa na Elena Grewal, mkurugenzi wa zamani wa sayansi ya data huko Airbnb, inaonyesha inaweza kuwa mara 100 zaidi, katika hali mbaya zaidi). Hii ni kabla ya kuzingatia athari kubwa kwenye mfumo wa matibabu, kama ile ilivyoelezwa hapo juu. Inafahamika kuwa watu wengine wanajaribu kujishawishi kwamba hii sio jambo jipya, ugonjwa kama mafua, kwa sababu sio vizuri sana kukubali ukweli kwamba hii sio kawaida kabisa. . Kujaribu kuelewa intuitively ukuaji wa kuongezeka kwa idadi ya watu walioambukizwa sio jambo ambalo akili zetu zimetengenezwa kushughulikia. Kwa hivyo inabidi tuchunguze hii kama wanasayansi, sio kutumia ubuni wetu. . Hii itakuwa wapi katika wiki 2? Miezi 2? . . Kwa kila mtu ambaye ana mafua, kwa wastani, huwaambukiza watu wengine 1.3. Hiyo inaitwa “R0” kwa homa. Ikiwa R0 ni chini ya 1.0, basi maambukizi huacha kuenea na kufa nje. Ikiwa ni zaidi ya 1.0, inaenea. R0 kwa sasa ni 2-3 kwa covid-19 nje ya Uchina. Tofauti inaweza kuonekana kuwa ndogo, lakini baada ya “vizazi” 20 vya watu walioambukizwa kupitisha maambukizo yao, R0 ya 1.3 inaweza kusababisha maambukizo 146, lakini R0 ya 2.5 inaweza kusababisha maambukizi milioni 36! (Hii ni kweli, ni ya mkono sana na inapuuza athari nyingi za ulimwengu, lakini ni mfano mzuri wa tofauti kati ya covid-19 na mafua, vitu vingine vyote kuwa sawa). . Kumbuka kuwa R0 sio mali ya msingi ya ugonjwa. Inategemea sana majibu, na inaweza kubadilika kwa wakati[2]. Kwa kushangaza zaidi, nchini China R0 kwa covid-19 imeshuka sana, na sasa inakaribia 1.0! Jinsi, unauliza? Kwa kuweka vipimo kwa kiwango ambacho itakuwa ngumu kufikiria katika nchi kama Amerika - kwa mfano, kufunga kabisa miji mikuu, na kuendeleza mchakato wa upimaji unaoruhusu watu zaidi ya milioni kwa wiki kupimwa. . Jambo moja ambalo huja kwenye media ya kijamii (pamoja na akaunti zilizofuatwa sana kama Elon Musk) ni kutokuelewana kwa tofauti kati ya mantiki na ufafanuzi ukuaji. Ukuaji wa mantiki unataja mfano wa ukuaji wa “umbo” la janga lililoenea katika mazoezi. Ni wazi ukuaji wa ukuaji hauwezi kuendelea milele, kwani sivyo kungekuwa na watu wengi walioambukizwa kuliko watu ulimwenguni! Kwa hivyo, mwishowe, viwango vya maambukizi lazima vimepungua kila wakati, na kusababisha kiwango cha ukuaji (kinachojulikana kama sigmoid) kwa wakati. Walakini, ukuaji unaopungua hujitokeza tu kwa sababu-sio uchawi. Sababu kuu ni: . Jibu kubwa na linalofaa la jamii, au. . | Asilimia kubwa kama hiyo ya watu wameambukizwa kwamba kuna watu wachache ambao hawajaambukizwa waeneze. . | . Kwa hivyo, haifanyi akili mantiki kutegemea muundo wa ukuaji wa vifaa kama njia ya “kudhibiti” janga. . Jambo lingine ambayo inafanya kuwa ngumu kuelewa athari za covid-19 katika jamii yako ni kwamba kuna kuchelewesha sana kati ya maambukizo na kulazwa hospitalini - kwa ujumla karibu siku 11. Hii inaweza kutoonekana kama muda mrefu, lakini ukilinganisha na idadi ya watu walioambukizwa wakati huo, inamaanisha kuwa kwa wakati utagundua kwamba vitanda vya hospitali vimejaa, maambukizi ya jamii tayari yapo katika kiwango ambacho kutakuwa na Mara 5-10 watu zaidi wa kushughulika nao. . Kumbuka kuwa kuna ishara kadhaa za mapema kuwa athari katika eneo lako inaweza angalau kutegemea hali ya hewa. karatsi Joto la joto na uchambuzi wa latitudo kutabiri kuenea kwa uwezo na msimu wa COVID-19 inabaini kuwa ugonjwa huo umeenea katika hali ya hewa kali (kwa bahati mbaya kwetu, kiwango cha joto huko San Francisco, tunakoishi, ni sawa katika safu hiyo ; pia inashughulikia vituo vikuu vya watu Ulaya, pamoja na London.) . “Usiogope. Tulia.” Haifai . Jibu moja la kawaida ambalo tumeona kwenye media ya kijamii kwa watu ambao wanaonyesha sababu za kuwa na wasiwasi, ni “usiogope” au “tulia”. Hii ni, kusema kidogo, sio msaada. Hakuna mtu anayependekeza kwamba hofu ni majibu sahihi. Kwa sababu fulani, hata hivyo, “utulivu” ni mwitiko maarufu katika duru fulani (lakini sio kati ya wataalam wa magonjwa yoyote, ambao kazi yao ni kufuatilia mambo haya). Labda “kutuliza” husaidia watu wengine kuhisi vizuri juu ya kutotenda kwao, au inawafanya wajisikie bora kwa watu ambao wanafikiria wanakimbiza kama kuku wasio na kichwa. . Lakini “kutuliza” kunaweza kusababisha urahisi kutofanikiwa kuandaa na kujibu. Huko Uchina, mamilioni ya waliwekwa chini na hospitali mbili mpya zilijengwa wakati walipofikia takwimu ambazo Amerika iko sasa. Italia ilisubiri muda mrefu sana, na leo tu (Jumapili Machi 8) waliripoti kesi mpya 1492 na vifo vipya 133, licha ya kuwafungia watu milioni 16. Kulingana na habari bora zaidi ambayo tunaweza kujua katika hatua hii, wiki mbili tu zilizopita Italia ilikuwa katika nafasi ile ile ambayo Amerika na Uingereza ziko leo (kwa njia ya takwimu za maambukizo). . Kumbuka kuwa karibu kila kitu kuhusu covid-19 katika hatua hii iko angani. Hatujui ni kasi ya maambukizi au vifo, hatujui ni muda gani inafanya kazi kwenye nyuso, hatujui ikiwa inaendelea kuishi na kusambaa katika hali ya joto. Kila kitu tulichonacho ni nadhani bora za sasa kulingana na habari bora watu wanaoweza kuweka pamoja. Kumbuka, idadi kubwa ya habari hii iko Uchina, kwa Kichina. Hivi sasa, njia bora ya kuelewa uzoefu wa Wachina hadi sasa ni kusoma Ripoti bora ya Ujumbe wa Pamoja wa WHO-China kuhusu ugonjwa wa Coronavirus 2019, kwa msingi wa dhamira ya pamoja ya wataalam 25 wa kitaifa na kimataifa kutoka Uchina, Ujerumani, Japan, Korea, Nigeria, Urusi, Singapore, Amerika ya Amerika na Shirika la Afya Duniani (WHO). . Wakati hakuna shaka, kwamba labda hii haitakuwa janga la ulimwengu, na labda kila kitu kinaweza kupita bila mfumo wa hospitali kuanguka, hiyo haimaanishi kuwa majibu sahihi sio kufanya chochote. Hiyo inaweza kuwa ya kufikiria sana na sio majibu sahihi chini ya hali yoyote ya mfano wa vitisho. Inaonekana pia kuwa uwezekano mkubwa kuwa nchi kama Italia na Uchina zinaweza kuziba sehemu kubwa za uchumi wao bila sababu nzuri. Sio pia sawa na athari halisi tunayoona ardhini katika maeneo yaliyoambukizwa, ambapo mfumo wa matibabu hauwezi kuhimili (kwa mfano, Italia inatumia hema 462 kwa “utangulizi”, na bado inabidi ihama wagonjwa wa ICU kutoka kwa maeneo yaliyoambukizwa). . Badala yake, mwitikio mzuri, wenye busara ni kufuata hatua ambazo zinapendekezwa na wataalam kuzuia kueneza maambukizo: . Epuka vikundi vikubwa na umati wa watu . | Ghairi matukio . | Fanya kazi kutoka nyumbani, ikiwa inawezekana . | Osha mikono unapokuja na kutoka nyumbani, na mara kwa mara wakati uko nje . | Epuka kugusa uso wako, haswa wakati uko nje ya nyumba yako (sio rahisi!) . | Disin uso na vifurushi (inawezekana virusi vinaweza kubaki kazi kwa siku 9 kwenye nyuso, ingawa hii bado haijajulikana kwa njia yoyote ile). . | . Sio juu yako tu . Ikiwa uko chini ya miaka 50, na hauna sababu za hatari kama mfumo wa kinga uliodhoofishwa, ugonjwa wa moyo na mishipa, historia ya uvutaji wa sigara uliopita, au magonjwa mengine sugu, basi unaweza kuwa na faraja kuwa covid-19 haiko uwezekano wa kukuua. Lakini jinsi unavyojibu bado ni mambo mengi sana. Bado una nafasi kubwa tu ya kuambukizwa, na ikiwa unafanya, nafasi kubwa tu ya kuambukiza wengine. Kwa wastani, kila mtu aliyeambukizwa anaambukiza zaidi ya watu wawili, na wanaambukiza kabla ya kuonyesha dalili. Ikiwa una wazazi unaowajali, au babu na babu, na unapanga kutumia wakati pamoja nao, na baadaye gundua kuwa una jukumu la kuwaambukiza na covid-19, hiyo itakuwa mzigo mzito kuishi nayo. . Hata ikiwa haukuwasiliana na watu zaidi ya miaka 50, kuna uwezekano kuwa una wafanyikazi wengi zaidi na marafiki wako wa kawaida na magonjwa sugu kuliko vile unavyogundua. Utafiti unaonyesha kuwa watu wachache huonyesha hali zao za kiafya mahali pa kazi ikiwa wanaweza kuizuia, kwa hofu ya kubaguliwa. Wote wawili wako katika jamii zilizo katika hatari kubwa, lakini watu wengi ambao tunaingiliana nao mara kwa mara wanaweza wasijue hii. . Na kwa kweli, sio tu juu ya watu wanaokuzunguka mara moja. Hili ni suala muhimu sana la maadili. Kila mtu anayefanya bidii yao kuchangia kudhibiti kuenea kwa virusi ni kusaidia jamii yao yote kupunguza kasi ya maambukizi. Kama Zeynep Tufekci aliandika katika Scientific American: “Kujiandaa kwa kuenea kwa uwezekano wa kuenea kwa virusi vya ulimwengu huu … ni moja wapo ya mambo ya kijamii, ya kujitolea ambayo unaweza kufanya”. Anaendelea: . Tunapaswa kuandaa, sio kwa sababu tunaweza kuhisi kibinafsi, lakini ili tuweze kusaidia kupunguza hatari kwa kila mtu. Tunapaswa kutayarisha sio kwa sababu tunakabiliwa na hali ya siku ya mwisho nje ya uwezo wetu, lakini kwa sababu tunaweza kubadilisha kila kipengele cha hatari hii tunayokabili kama jamii. Hiyo ni kweli, unapaswa kujiandaa kwa sababu majirani zako wanahitaji utayarishe-haswa majirani wako wazee, majirani zako wanaofanya kazi hospitalini, majirani zako walio na magonjwa sugu, na majirani ambao wanaweza kukosa njia au wakati wa kuandaa kwa sababu ya kukosa rasilimali au wakati. . Hii imeathiri sisi kibinafsi. Kozi kubwa na muhimu zaidi ambayo tumewahi kuunda fast.ai, ambayo inawakilisha mwisho wa miaka ya kazi kwa ajili yetu, ilipangwa kuanza katika Chuo Kikuu cha San Francisco katika wiki. Jumatano iliyopita (Machi 4), tulifanya uamuzi wa kuhamisha kitu hicho mkondoni. Tulikuwa moja ya kozi kubwa ya kwanza kuhamia mkondoni. Kwa nini tulifanya? Kwa sababu tuligundua mapema wiki iliyopita kwamba ikiwa tungeendesha kozi hii, tunawahimiza sana mamia ya watu kuungana katika nafasi iliyofungwa, mara kadhaa kwa kipindi cha wiki nyingi. Kuleta vikundi pamoja katika nafasi zilizofungwa ni jambo moja mbaya zaidi ambalo linaweza kufanywa. Tulihisi tulazimishwa kiakili kuhakikisha kuwa, angalau katika kesi hii, hii haikutokea. Ilikuwa uamuzi wa kuvunja moyo. Wakati wetu uliotumiwa kufanya kazi moja kwa moja na wanafunzi wetu imekuwa moja ya starehe nzuri na vipindi vingi vya uzalishaji kila mwaka. Na tulikuwa na wanafunzi waliopanga kuruka kutoka ulimwenguni kote, ambao hatutaki kumruhusu[3]. . Lakini tulijua ni jambo sahihi kufanya, kwa sababu sivyo tutaweza kuwa tunazidisha kuenea kwa ugonjwa huo katika jamii yetu[4]. . Tunahitaji kueneza Curve . Hii ni muhimu sana, kwa sababu ikiwa tunaweza kupunguza kasi ya maambukizi katika jamii, basi tunawapa hospitali katika jamii hiyo wakati wa kukabiliana na wagonjwa wote walioambukizwa, na mzigo wa kawaida wa wagonjwa wanaohitaji kushughulikia. Hii inaelezewa kama “kubatilisha curve”, na imeonyeshwa wazi katika chati hii ya kuonyesha: . Kukaa chini ya mstari huo ulio na alama kunamaanisha kila kitu . . Farzad Mostashari, Mratibu wa Zamani wa Afya ya IT, alielezea: “Kesi mpya zinatambuliwa kila siku ambazo hazina historia ya kusafiri au kiunga cha kesi inayojulikana, na tunajua kuwa hizi ni ncha za barafu kwa sababu ya ucheleweshaji katika kupima. Hiyo inamaanisha kuwa katika wiki mbili zijazo idadi ya watu wanaotambuliwa italipuka…. Kujaribu kufanya jambo wakati kuna kuenea kwa jamii ni kama kuzingatia kuweka cheche wakati nyumba iko moto. Wakati hiyo ikifanyika, tunahitaji kubadili mikakati ya kupunguza - kuchukua hatua za kinga ili polepole kuenea na kupunguza athari za kilele kwa huduma ya afya. “ Ikiwa tunaweza kuweka kuenea kwa magonjwa chini ya kwamba hospitali zetu zinaweza kushughulikia mzigo, basi watu wanaweza kupata matibabu. Lakini ikiwa kesi zinakuja haraka sana, basi zile ambazo zinahitaji kulazwa hazitaipata. . Hii ndio hesabu inaweza kuonekana, kulingana na Liz Specht: . Amerika ina vitanda takriban 2.8 vya hospitali kwa kila watu 1000. Pamoja na idadi ya watu 330M, hii ni vitanda vya ~ 1M. Kwa wakati wowote, 65% ya vitanda hivyo vimeshaa. Hiyo inaacha vitanda 330k vinavyopatikana kote nchini (labda kidogo wakati huu wa mwaka na msimu wa homa ya kawaida, nk). Wacha tuamini idadi ya Italia na kudhani kuwa karibu 10% ya kesi ni kubwa za kutosha kuhitaji kulazwa hospitalini. (Kumbuka kwamba kwa wagonjwa wengi, kulazwa hospitalini kwa wiki - kwa maneno mengine, mauzo yatakuwa polepole sana kwani vitanda hujaza wagonjwa wa COVID19). Kwa makisio haya, kufikia Mei 8, vitanda vyote vya wazi vya hospitali nchini Amerika vitajazwa. (Hii haisemi chochote, kwa kweli, juu ya ikiwa vitanda hivi vinafaa kutengwa kwa wagonjwa walio na virusi vya kuambukiza sana.) Ikiwa tunakosea kwa sababu ya mbili kuhusu sehemu ya kesi kali, hiyo inabadilisha tu ratiba ya muda wa kueneza kitanda. kwa siku 6 kwa pande zote mbili. Ikiwa 20% ya kesi zinahitaji kulazwa hospitalini, tunamaliza vitanda na ~ 2 Mei Ikiwa tu 5% ya kesi zinahitaji, tunaweza kuifanya hadi ~ Mei 14. 2.5% inatupeleka Mei 20. Hii, kwa kweli, inadhani kwamba hakuna uvumbuzi wa mahitaji ya vitanda kutoka kwa sababu zingine (zisizo za COVID19), ambayo inaonekana kama dhana mbaya. Wakati mfumo wa utunzaji wa afya unazidi kuwa mzito, uhaba wa Rx, nk, watu w / hali sugu ambazo husimamiwa vizuri huweza kujikuta wakitumbukia katika majimbo makali ya dhiki ya kitabibu inayohitaji utunzaji mkubwa na kulazwa hospitalini. . Mwitikio wa jamii hufanya tofauti zote . Kama tulivyojadili, hesabu hii sio ukweli- Uchina tayari imeonyesha kuwa inawezekana kupunguza kuenea kwa kuchukua hatua kali. Mfano mwingine mzuri wa mwitikio uliofanikiwa ni Vietnam, ambapo, kati ya mambo mengine, kampeni ya matangazo ya kitaifa (pamoja na wimbo wa kuvutia!) Ilichochea majibu ya jamii haraka na kuhakikisha kuwa watu wanabadilisha tabia zao ipasavyo. . Hii sio hali ya kiakili tu - ilionyeshwa wazi katika janga la mafua la 1918. Huko Merika miji miwili ilionyesha athari tofauti sana kwa janga hili: Philadelphia ilienda mbele na gwaride kubwa la watu 200,000 kusaidia kuongeza pesa kwa vita. Lakini St Louis aliweka michakato iliyoundwa kwa uangalifu ili kupunguza mawasiliano ya kijamii ili kupungua kwa kuenea kwa virusi, pamoja na kufuta hafla zote kubwa. Hapa ndivyo idadi ya vifo ilionekana katika kila mji, kama inavyoonyeshwa kwenye Taratibu za National Academy of Sciences: . Matokeo ya majibu tofauti kwa janga la mafua ya 1918 . . Hali ya Philadelphia ilizidi kuwa mbaya sana, hata kufikia mahali ambapo hakukuwa na pakiti za mazishi za kutosha au mikato ya kushughulikia idadi kubwa ya waliokufa kutokana na homa hiyo. . Richard Besser, ambaye alikuwa kaimu mkurugenzi wa Vituo vya Kudhibiti na Kuzuia Magonjwa wakati wa janga la H1N1 la 2009, anasema kwamba nchini Merika “hatari ya kufichuliwa na uwezo wa kujilinda na familia ya mtu inategemea mapato, ufikiaji wa huduma za afya, na hali ya uhamiaji, miongoni mwa sababu zingine. “ Anaonyesha kuwa: . Wazee na walemavu wako katika hatari fulani wakati maisha yao ya kila siku na mifumo ya msaada inavurugika. Wale wasio na huduma rahisi ya huduma za afya, pamoja na jamii za vijijini na Native, wanaweza kukabiliwa na umbali mzito wakati wa shida. Watu wanaoishi katika nyumba za karibu - iwe katika makazi ya umma, nyumba za wauguzi, magereza, malazi au hata wasio na makazi mitaani - wanaweza kuteseka kwa mawimbi, kama vile tumeona tayari katika jimbo la Washington. Na udhaifu wa uchumi wa gge wa mshahara wa chini, na wafanyikazi wasio na mishahara na ratiba za kazi ngumu, utafunuliwa kwa wote kuona wakati wa shida hii. Uliza asilimia 60 ya nguvu kazi ya Merika ambayo hulipwa saa moja jinsi ni rahisi kuchukua wakati katika hitaji. . Ofisi ya Takwimu ya Kazi ya Amerika inaonyesha kwamba chini ya theluthi ya wale walio kwenye bendi ya kipato cha chini wanapata likizo ya mgonjwa ya kulipwa: . Wamarekani wengi masikini hawana likizo ya kuugua, kwa hivyo lazima uende kazini. . . Hatuna habari nzuri huko Amerika . Mojawapo ya maswala makubwa nchini Merika ni kwamba upimaji mdogo sana unafanywa, na matokeo ya upimaji hayashirikiwi vizuri, ambayo inamaanisha hatujui kile kinachotokea. Scott Gottlieb, kamishna wa zamani wa FDA, alielezea kwamba huko Seattle kumekuwa na majaribio bora, na tunaona maambukizo huko: “Sababu ya sisi kujua mapema juu ya kuzuka kwa Seattle ya covid-19 ni kwa sababu ya kazi ya uchunguzi wa wanasayansi huru. Uchunguzi kama huo haujawahi kuendelea kabisa katika miji mingine. Kwa hivyo maeneo mengine moto ya Merika bado hayawezi kugunduliwa. “ Kulingana na The Atlantic, Makamu wa Rais, Mike Pence aliahidi kwamba “takriban vipimo milioni 1.5” vitapatikana wiki hii, lakini watu wasiopungua 2000 wamejaribiwa kote Amerika kwa sasa. Kuchora kazi kutoka Mradi wa COVID Tracking Project, Robinson Meyer na Alexis Madrigal wa Atlantic, walisema: . Takwimu tulizokusanya zinaonyesha kuwa mwitikio wa Amerika kwa covid-19 na ugonjwa unaosababisha, COVID-19, imekuwa ya uvivu wa kushangaza, haswa ikilinganishwa na ile ya nchi nyingine zilizoendelea. CDC ilithibitisha siku nane zilizopita kwamba virusi hivyo vilikuwa katika maambukizi ya jamii huko Merika-kwamba ilikuwa ikiambukiza Wamarekani ambao walikuwa hawajasafiri kwenda nje ya nchi na hawakuwasiliana na wengine ambao walikuwa. Huko Korea Kusini, zaidi ya watu 66,650 walijaribiwa katika wiki moja ya kesi ya kwanza ya maambukizi ya jamii, na mara moja iliweza kujaribu watu 10,000 kwa siku. . Sehemu ya shida ni kwamba hii imekuwa suala la kisiasa. Hasa, Rais Donald Trump ni mfano wa ambapo utaftaji wa madini unaingilia kati kupata matokeo mazuri katika mazoezi. (Kwa zaidi juu ya suala hili, tazama Maadili ya Karatasi ya Sayansi ya Tatizo Shida na Metric ni Tatizo la AI kwa msingi). Mkuu wa Google wa AI Jeff Dean, tweeted juu ya shida za disinformation za kisiasa: . Wakati nilifanya kazi kwa WHO, nilikuwa sehemu ya Programu ya Ulimwenguni juu ya UKIMWI (sasa UNAIDS), iliyoundwa ili kusaidia ulimwengu kukabiliana na janga la VVU / UKIMWI. Wafanyikazi hapo walikuwa madaktari na wanasayansi waliojitolea sana kulenga kushughulikia shida hiyo. Katika nyakati za shida, habari wazi na sahihi ni muhimu kusaidia kila mtu kufanya maamuzi sahihi na yenye habari juu ya jinsi ya kujibu (nchi, serikali, na serikali za mitaa, kampuni, NGO, shule, familia, na watu binafsi). Pamoja na habari na sera sahihi zilizowekwa kwa ajili ya kusikiliza wataalam bora wa matibabu na wanasayansi, sote tutakuja kupitia changamoto kama zile zilizowasilishwa na VVU / UKIMWI au COVID-19. Pamoja na utaftaji unaoendeshwa na masilahi ya kisiasa, kuna hatari ya kweli ya kufanya mambo kuwa mbaya zaidi, kwa kutofanya haraka na kwa dhati mbele ya janga linalokua, na kwa kutia moyo tabia ambazo zitasambaza ugonjwa haraka. Hali hii nzima ni chungu sana kutazama ikitokea. . Haionekani kana kwamba kuna matakwa ya kisiasa ya kugeuza mambo, inapofikia uwazi. Katibu wa Huduma za Afya na Binadamu, Alex Azar, kulingana na Wired, “alianza kuzungumza juu ya vipimo ambavyo wafanyikazi wa huduma ya afya hutumia kubaini ikiwa mtu ameambukizwa na ugonjwa mpya. Ukosefu wa vifaa hivyo unamaanisha ukosefu mkubwa wa habari ya ugonjwa kuhusu kuenea na ukali wa ugonjwa huo huko Merika, uliozidishwa na opacity kwa upande wa serikali. Azar alijaribu kusema kuwa majaribio zaidi yalikuwa njiani, yanasubiri udhibiti wa ubora. “ Lakini, waliendelea: . Kisha Trump akamkata Azar. “Lakini nadhani, muhimu, mtu yeyote, hivi sasa na jana, kwamba anahitaji mtihani anapata mtihani. Wapo, wana vipimo, na vipimo ni nzuri. Mtu yeyote anayehitaji mtihani anapata mtihani, “Trump alisema. Huo sio ukweli. Makamu wa Rais Pence aliwaambia waandishi wa habari Alhamisi kuwa Amerika haina vifaa vya kutosha vya kukidhi mahitaji. . Nchi zingine zinajibu haraka sana na kwa kiwango kikubwa kuliko Amerika. Nchi nyingi katika SE Asia zinaonyesha matokeo mazuri, pamoja na Taiwan, ambapo R0 iko chini hadi 0.3 sasa, na Singapore, ambayo inapendekezwa kama The Model for COVID-19 Response. Sio tu kwa Asia; kwa Ufaransa, kwa mfano, mkusanyiko wowote wa watu 1000 ni marufuku, na shule sasa zimefungwa katika wilaya tatu. . Hitimisho . Covid-19 ni suala muhimu la kijamii, na tunaweza, na tunapaswa, wote kufanya kazi kupungua kwa kuenea kwa ugonjwa huo. Hii inamaanisha: . Kuepuka vikundi vikubwa na umati wa watu . | Inaghairi matukio . | Kufanya kazi kutoka nyumbani, ikiwa inawezekana . | Kuosha mikono wakati unakuja na kutoka nyumbani, na mara kwa mara wakati uko nje . | Kuepuka kugusa uso wako, haswa ukiwa nje ya nyumba yako. . | . Kumbuka: kwa sababu ya dharura ya kupata hayo, tumekuwa sio waangalifu kama kawaida tunapenda kuwa juu ya kuhutubia na kuashiria kazi tunayotegemea. Tafadhali tujulishe ikiwa tumekosa chochote. . Asante kwa Sylvain Gugger na Alexis Gallagher kwa maoni na maoni. . Maelezo ya chini . 1. Epidemiologists ni watu ambao husoma kuenea kwa magonjwa. Inabadilika kuwa makadirio ya vitu kama vifo na R0 ni changamoto nzuri kweli, kwa hivyo kuna uwanja mzima ambao utaalam katika kufanya hivi vizuri. Kuwa mwangalifu na watu ambao hutumia uhesabu rahisi na takwimu kukuambia jinsi covid-19 inavyoendelea. Badala yake, angalia mfano unaofanywa na wataalam wa magonjwa ya magonjwa . 2. Kweli, sio kweli kitaalam. “R0” kuzungumza madhubuti inahusu kiwango cha maambukizi kwa kukosekana kwa majibu. Lakini kwa kuwa hiyo sio jambo ambalo tunawajali sana, tunajiruhusu kuwa kidogo wepesi juu ya ufafanuzi wetu hapa . 3. Tangu uamuzi huo, tumejitahidi kupata njia ya kuendesha kozi halisi ambayo tunatumai itakuwa bora zaidi kuliko toleo la mtu mwenyewe ingekuwa. Tumeweza kuifungua kwa mtu yeyote ulimwenguni, na tutakuwa tukifanya uchunguzi wa kawaida na vikundi vya miradi kila siku . 4. Tumefanya mabadiliko mengine madogo kwa mtindo wetu wa maisha pia, pamoja na mazoezi nyumbani badala ya kwenda kwenye mazoezi, kusonga mikutano yetu yote kwenye mkutano wa video, na kuruka hafla za usiku ambazo tumekuwa tukitazamia .",
            "url": "https://asvcode.github.io/Blogs/coronavirus/covid19/sawhili/kiswahili/2020/03/10/Covid-19-jamii-yako-na-wewe-mtazamo-wa-sayansi-ya-data.html",
            "relUrl": "/coronavirus/covid19/sawhili/kiswahili/2020/03/10/Covid-19-jamii-yako-na-wewe-mtazamo-wa-sayansi-ya-data.html",
            "date": " • Mar 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This site is built with fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. . . fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. A full list of features can be found on GitHub. .",
          "url": "https://asvcode.github.io/Blogs/fastpages/",
          "relUrl": "/fastpages/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}