{
  
    
        "post0": {
            "title": "Medical Imaging Starter",
            "content": "Goal: . The goals of this starter notebook are to: . use DICOMs as the image input | high level overview of what considerations need to be taken and what the results mean when creating a model that predicts medical conditions | . The dataset used is conveniently provided by fastai - SIIM-ACR Pneumothorax Segmentation dataset and contains 250 Dicom images (175 No Pneumothorax and 75 Pneumothorax) . Considerations: . patient overlap | sampling | evaluting AI models for medical use | . This notebook is based on this fastai notebook. For more information about DICOMs and fastai medical imaging you can click here . Load the Data . pneumothorax_source = untar_data(URLs.SIIM_SMALL) items = get_dicom_files(pneumothorax_source, recurse=True, folders=&#39;sm&#39;) df = pd.read_csv(pneumothorax_source/f&quot;labels_sm.csv&quot;) . items . (#26) [Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000000 - Copy.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000000.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000002.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000005.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000006 - Copy.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000006.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000007.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000008.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000009.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/sm/No Pneumothorax/000011.dcm&#39;)...] . Side Note: The SIIM_SMALL dataset has no duplicate patient IDs, has an equal number of males and females so I used a custom even smaller dataset to show the functionality of DicomSplit and DataSplit below . Viewing the Data . View Dicom . The show function is specifically tailored to display .dcm formats. By customizing the show function we have now view patient information with each image . @patch @delegates(show_image) def show_dinfo(self:DcmDataset, scale=True, cmap=plt.cm.bone, min_px=-1100, max_px=None, **kwargs): &quot;&quot;&quot;show function that prints patient attributes from DICOM head&quot;&quot;&quot; px = (self.windowed(*scale) if isinstance(scale,tuple) else self.hist_scaled(min_px=min_px,max_px=max_px,brks=scale) if isinstance(scale,(ndarray,Tensor)) else self.hist_scaled(min_px=min_px,max_px=max_px) if scale else self.scaled_px) print(f&#39;Patient Age: {self.PatientAge}&#39;) print(f&#39;Patient Sex: {self.PatientSex}&#39;) print(f&#39;Body Part Examined: {self.BodyPartExamined}&#39;) print(f&#39;Rows: {self.Rows} Columns: {self.Columns}&#39;) show_image(px, cmap=cmap, **kwargs) . patient = 7 sample = dcmread(items[patient]) sample.show_dinfo() . Patient Age: 31 Patient Sex: M Body Part Examined: CHEST Rows: 1024 Columns: 1024 . Create a Dataframe . DICOM formats contain alot of useful information but difficult to see image by image so we need to capture this information and create a dataframe for better viewing and data manipulation. . Create a dataframe Customize the functions so that we include what we want in our dataframe . #updating to accomodate def _dcm2dict2(fn, **kwargs): t = fn.dcmread() return fn, t.PatientID, t.PatientAge, t.PatientSex, t.BodyPartExamined, t.Modality, t.Rows, t.Columns, t.BitsStored, t.PixelRepresentation @delegates(parallel) def _from_dicoms2(cls, fns, n_workers=0, **kwargs): return pd.DataFrame(parallel(_dcm2dict2, fns, n_workers=n_workers, **kwargs)) pd.DataFrame.from_dicoms2 = classmethod(_from_dicoms2) . test_df = pd.DataFrame.from_dicoms2(items) test_df.columns=[&#39;file&#39;, &#39;PatientID&#39;, &#39;Age&#39;, &#39;Sex&#39;, &#39;Bodypart&#39;, &#39;Modality&#39;, &#39;Rows&#39;, &#39;Cols&#39;, &#39;BitsStored&#39;, &#39;PixelRep&#39; ] test_df.to_csv(&#39;test_df.csv&#39;) test_df.head() . file PatientID Age Sex Bodypart Modality Rows Cols BitsStored PixelRep . 0 C: Users avird .fastai data siim_small sm No Pneumothorax 000000 - Copy.dcm | 16d7f894-55d7-4d95-8957-d18987f0e981 | 62 | M | CHEST | CR | 1024 | 1024 | 8 | 0 | . 1 C: Users avird .fastai data siim_small sm No Pneumothorax 000000.dcm | 16d7f894-55d7-4d95-8957-d18987f0e981 | 62 | M | CHEST | CR | 1024 | 1024 | 8 | 0 | . 2 C: Users avird .fastai data siim_small sm No Pneumothorax 000002.dcm | 850ddeb3-73ac-45e0-96bf-7d275bc83782 | 52 | F | CHEST | CR | 1024 | 1024 | 8 | 0 | . 3 C: Users avird .fastai data siim_small sm No Pneumothorax 000005.dcm | e0fd6161-2b8d-4757-96bc-6cf620a993d5 | 65 | F | CHEST | CR | 1024 | 1024 | 8 | 0 | . 4 C: Users avird .fastai data siim_small sm No Pneumothorax 000006 - Copy.dcm | 99171908-3665-48e8-82c8-66d0098ce209 | 52 | F | CHEST | CR | 1024 | 1024 | 8 | 0 | . We can now view the information (note this for my custom dataset) . #Plot 3 comparisons def plot_comparison(df, feature, feature1, feature2): &quot;Plot 3 comparisons from a dataframe&quot; fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (16, 4)) s1 = sns.countplot(df[feature], ax=ax1) s1.set_title(feature) s2 = sns.countplot(df[feature1], ax=ax2) s2.set_title(feature1) s3 = sns.countplot(df[feature2], ax=ax3) s3.set_title(feature2) plt.show() . plot_comparison(test_df, &#39;PatientID&#39;, &#39;Sex&#39;, &#39;Bodypart&#39;) . Age comparison . def age_comparison(df, feature): &quot;Plot hisogram of age range in dataset&quot; fig, (ax1) = plt.subplots(1,1, figsize = (16, 4)) s1 = sns.countplot(df[feature], ax=ax1) s1.set_title(feature) plt.show() age_comparison(test_df, &#39;Age&#39;) . Modelling . Considerations: . patient overlap between train and val set | sampling - how many negative and postive cases are in the train/val split (class imbalance) | augmentations - consideration of what augmentations are used and why in some cases may not be useful | . Patient Overlap . It is important to know if there is going to be any patient overlap when creating the train and validation sets as this may lead to an overly optimistic test set. The great thing about DICOMs is that we can check to see if there are any duplicate patientIDs in the test and valid sets when we split our data . def DicomSplit(valid_pct=0.2, seed=None, **kwargs): &quot;Splits `items` between train/val with `valid_pct`&quot; &quot;and checks if identical patient IDs exist in both the train and valid sets&quot; def _inner(o, **kwargs): train_list=[]; valid_list=[] if seed is not None: torch.manual_seed(seed) rand_idx = L(int(i) for i in torch.randperm(len(o))) cut = int(valid_pct * len(o)) trn = rand_idx[cut:]; trn_p = o[rand_idx[cut:]] val = rand_idx[:cut]; val_p = o[rand_idx[:cut]] for i, im in enumerate(trn_p): trn = im.dcmread() patient_ID = trn.PatientID train_list.append(patient_ID) for j, jm in enumerate(val_p): val = jm.dcmread() vpatient_ID = val.PatientID valid_list.append(vpatient_ID) print(set(train_list) &amp; set(valid_list)) return rand_idx[cut:], rand_idx[:cut] return _inner . set_seed(7) trn,val = DicomSplit(valid_pct=0.2)(items) trn, val . {&#39;6224213b-a185-4821-8490-c9cba260a959&#39;} . ((#21) [2,13,9,12,11,24,8,14,16,6...], (#5) [19,18,3,23,17]) . The custom test dataset only has 26 images which is split in a test set of 24 and a valid set of 5 using valid_pct of 0.2. By customizing RandomSplitter into DicomSplit you can view to see if there are any duplicate PatientIDs. In this case there is a duplicate ID: 6224213b-a185-4821-8490-c9cba260a959 . Using set_seed allows for reproducible results and ensures we use the same seed when training . Sampling . This dataset has 2 classes Pneumothorax and No Pneumothorax, DataSplit looks at how many Pneumothorax and No Pneumothorax images are in the train and valid sets. This is to view how fair the train/val split is to ensure good model sampling . def DataSplit(valid_pct=0.2, seed=None, **kwargs): &quot;Check the number of each class in train and valid sets&quot; def _inner(o, **kwargs): train_list=[]; valid_list=[] if seed is not None: torch.manual_seed(seed) rand_idx = L(int(i) for i in torch.randperm(len(o))) cut = int(valid_pct * len(o)) trn_p = o[rand_idx[cut:]] val_p = o[rand_idx[:cut]] for p in enumerate(trn_p): b = str(p).split(&#39;/&#39;)[7] train_list.append(b) for q in enumerate(val_p): e = str(q).split(&#39;/&#39;)[7] valid_list.append(e) print(f&#39;train: {train_list} n valid: {valid_list}&#39;) return rand_idx[cut:], rand_idx[:cut] return _inner . using the same set_seed we can get reproducible results . set_seed(7) trn,val = DataSplit(valid_pct=0.2)(items) trn, val . train: [&#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;No Pneumothorax&#39;] valid: [&#39;Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;, &#39;Pneumothorax&#39;, &#39;No Pneumothorax&#39;] . ((#21) [2,13,9,12,11,24,8,14,16,6...], (#5) [19,18,3,23,17]) . With this test dataset (the train set has 14 No Pneumothorax and 5 Pneumothorax images and the valid set has 4 No Pneumothorax and 3 Pneumothorax images . &gt;&gt;Work in progress work on various techniques to help with unbalanced datasets especially true for medical image datasets where there are typically alot more images of &#39;normal&#39; compared to &#39;diseased&#39; (oversampling, k-fold) . Augmentations . img1 = (pneumothorax_source/&#39;chest1.png&#39;); img2 = (pneumothorax_source/&#39;chest2.png&#39;) . &gt;&gt;Work in progress Choosing the right augmentations is important in determing how it affects the sampling process. For example in some cases it may not be a good idea to flip images. . Here is an image of a &#39;normal&#39; patient in its correct orientation (heart showing in the middle right) . Image.open(img1) . If we flip the image . Image.open(img2) . We can now see the heart is middle left. If the classifier was looking to detect defects of the heart then this type of augmentation would not be suitable. . #clear out some memory import gc gc.collect() . 33619 . #switch back to the full dataset items = get_dicom_files(pneumothorax_source, recurse=True, folders=&#39;train&#39;) df = pd.read_csv(pneumothorax_source/f&quot;labels.csv&quot;) . xtra_tfms = [RandomResizedCrop(224)] batch_tfms = [*aug_transforms(do_flip=False, flip_vert=False, xtra_tfms=xtra_tfms), Normalize.from_stats(*imagenet_stats)] . set_seed(7) pneumothorax = DataBlock(blocks=(ImageBlock(cls=PILDicom), CategoryBlock), get_x=lambda x:pneumothorax_source/f&quot;{x[0]}&quot;, get_y=lambda x:x[1], splitter=RandomSplitter(valid_pct=0.3), item_tfms=Resize(256), batch_tfms=batch_tfms) dls = pneumothorax.dataloaders(df.values, bs=12, num_workers=0) . Check train and valid sizes . len(dls.train_ds), len(dls.valid_ds) . (175, 75) . precision = Precision() recall = Recall() . net = xresnext18(pretrained=False, sa=True, act_cls=Mish, n_out=dls.c) learn = Learner(dls, net, metrics=[accuracy, precision, recall], cbs=[ShowGraphCallback(), CollectDataCallback]) . If you do not specifiy a loss function or optimization function fastai automatically allocates one. In this example using defaults . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . learn.opt_func . &lt;function fastai2.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)&gt; . learn.lr_find() . SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=0.0014454397605732083) . learn.freeze() learn.fit_one_cycle(1, slice(3e-3)) . epoch train_loss valid_loss accuracy precision_score recall_score time . 0 | 0.715601 | 0.857002 | 0.666667 | 0.307692 | 0.200000 | 00:08 | . learn.unfreeze() learn.fit_one_cycle(5, slice(3e-3)) . epoch train_loss valid_loss accuracy precision_score recall_score time . 0 | 0.671033 | 0.764320 | 0.493333 | 0.304348 | 0.700000 | 00:08 | . 1 | 0.674126 | 1.674710 | 0.293333 | 0.260870 | 0.900000 | 00:08 | . 2 | 0.664581 | 0.742959 | 0.733333 | 0.000000 | 0.000000 | 00:08 | . 3 | 0.633107 | 0.670259 | 0.640000 | 0.315789 | 0.300000 | 00:08 | . 4 | 0.617154 | 0.619417 | 0.693333 | 0.333333 | 0.150000 | 00:08 | . C: Users avird Anaconda3 envs fastmed lib site-packages sklearn metrics _classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . def show_results2(self, ds_idx=1, dl=None, max_n=9, shuffle=False, **kwargs): if dl is None: dl = self.dls[ds_idx].new(shuffle=shuffle) b = dl.one_batch() t,a,preds = self.get_preds(dl=[b], with_decoded=True) print(f&#39;Acutal: {a} n Preds: {preds} n&#39;) self.dls.show_results(b, preds, max_n=max_n, **kwargs) . show_results2(learn, max_n=12, nrows=2, ncols=6) . Acutal: TensorCategory([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1], dtype=torch.int32) Preds: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]) . Model Evaluation . Because medical models are high impact it is important to know how good a model is at detecting a certain condition. . Accuracy . The above model has an accuracy of 69%. One needs to look deeper into how the accuracy of 69% was calculated and whether it is acceptable. . Accuracy is the probablity that the model is correct or | . Accuracy is the probability that the model is correct and the patient has the condition PLUS the probability that the model is correct and the patient does not have the condition | . False Positive &amp; False Negative . There are some other key terms that need to be used when evaluating medical models: . False Negative is an error in which a test result improperly indicates no presence of a condition (the result is negative), when in reality it is present. | . False Positive is an error in which a test result improperly indicates presence of a condition, such as a disease (the result is positive), when in reality it is not present | . Sensitivity &amp; Specificity . Sensitivity or True Positive Rate is where the model classifies a patient has the disease given the patient actually does have the disease. Sensitivity quantifies the avoidance of false negatives | . Example: A new test was tested on 10,000 patients, if the new test has a sensitivity of 90% the test will correctly detect 9,000 (True Positive) patients but will miss 1000 (False Negative) patients that have the condition but were tested as not having the condition . Specificity or True Negative Rate is where the model classifies a patient as not having the disease given the patient actually does not have the disease. Specificity quantifies the avoidance of false positives | . Understanding and using sensitivity, specificity and predictive values is a great paper if you are interested in learning more . PPV and NPV . Most medical testing is evaluated via PPV (Postive Predictive Value) or NPV (Negative Predictive Value). . PPV - if the model predicts a patient has a condition what is probabilty that the patient actually has the condition . | NPV - if the model predicts a patient does not have a condition what is the probability that the patient actually does not have the condition . | . The ideal value of the PPV, with a perfect test, is 1 (100%), and the worst possible value would be zero . The ideal value of the NPV, with a perfect test, is 1 (100%), and the worst possible value would be zero . Confusion Matrix . Plot a confusion matrix - note that this is plotted against the valid dataset size which is 75 in this case . interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() len(dls.valid_ds)==len(losses)==len(idxs) interp.plot_confusion_matrix(figsize=(7,7)) . upp, low = interp.confusion_matrix() tn, fp = upp[0], upp[1] fn, tp = low[0], low[1] print(tn, fp, fn, tp) . 49 6 17 3 . Sensitivity = True Positive/(True Positive + False Negative) . sensitivity = tp/(tp + fn) sensitivity . 0.15 . In this case the model only has a sensitivity of 15% and hence is only capable of correctly detecting 15% True Positives(ie who have Pneumothorax) but will miss 85% of False Negatives (patients that actually have Pneumothorax but were told they did not! Not a good situation to be in). . This is also know as a Type II error . Specificity = True Negative/(False Positive + True Negative) . specificity = tn/(fp + tn) specificity . 0.8909090909090909 . In this case the model has a specificity of 89% and hence can correctly detect 89% of the time that a patient does not have Pneumothorax but will incorrectly classify that 11% of the patients have Pneumothorax (False Postive) but actually do not. . This is also known as a Type I error . Positive Predictive Value (PPV) . ppv = tp/(tp+fp) ppv . 0.3333333333333333 . In this case the model performs poorly in correctly predicting patients with Pneumothorax . Negative Predictive Value (NPV) . npv = tn/(tn+fn) npv . 0.7424242424242424 . This model is better at predicting patients with No Pneumothorax . Some of these metrics can be calculated using sklearn&#39;s classification report . interp.print_classification_report() . precision recall f1-score support No Pneumothorax 0.74 0.89 0.81 55 Pneumothorax 0.33 0.15 0.21 20 accuracy 0.69 75 macro avg 0.54 0.52 0.51 75 weighted avg 0.63 0.69 0.65 75 . Calculating Accuracy . The accuracy of this model as mentioned before is 69% - lets now calculate this! . We can also look at Accuracy as: . accuracy = sensitivity prevalence + specificity (1 - prevalence) . Prevalence is a statistical concept referring to the number of cases of a disease that are present in a particular population at a given time. . The prevalence in this case is how many patients in the valid dataset have the condition compared to the total number. To view the number of Pneuomothorax patients in the valid set . t= dls.valid_ds.cat #t[0] . There are 20 Pneumothorax images in the valid set hence the prevalance here is 20/75 = 0.27 . accuracy = (0.15 * 0.27) + (0.89 * (1 - 0.27)) accuracy . 0.6901999999999999 . The question is, is this model ready for medical use? . fin .",
            "url": "https://asvcode.github.io/Blogs/medical_imaging/dicom/model_evaluation/ppv/npv/specificity/sensitivity/2020/04/29/Medical-Imaging-Starter.html",
            "relUrl": "/medical_imaging/dicom/model_evaluation/ppv/npv/specificity/sensitivity/2020/04/29/Medical-Imaging-Starter.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Medical Imaging Part2",
            "content": "Continuing from Part1 &gt;&gt;&gt;&gt; . from fastai2.basics import * from fastai2.callback.all import * from fastai2.vision.all import * from fastai2.medical.imaging import * import pydicom matplotlib.rcParams[&#39;image.cmap&#39;] = &#39;bone&#39; pneumothorax_source = untar_data(URLs.SIIM_SMALL) items = get_dicom_files(pneumothorax_source, recurse=True, folders=&#39;train&#39;) img = items[10] dimg = dcmread(img) . Class TensorCTScan . Inherits from PILBase . Class PILCTScan . Inherits from PILBase . Dataset.pct_in_window . Gets the % of pixels within a window w, l . For example using lungs as the dicom_window (1500,-600) we see that 80% of the pixels are within the window . dimg.pct_in_window(*dicom_windows.lungs) . 0.7996940612792969 . uniform_blur2d . #Todo . ims = dimg.hist_scaled(), uniform_blur2d(dimg.hist_scaled(),50) show_images(ims, titles=(&#39;orig&#39;, &#39;blurred&#39;), figsize=(10,10)) . gauss_blur2d . #Todo . Uses gaussian_blur2d kornia filter . gims = dimg.hist_scaled(), gauss_blur2d(dimg.hist_scaled(),50) show_images(gims, titles=(&#39;orig&#39;, &#39;gauss_blurred&#39;), figsize=(10,10)) . As explained in this notebook Jeremy uses these methods in cleaning the data in order to only retain information that is relevant. You use gauss_blur2d to blur the image and select the areas that are bright . px = dimg.windowed(*dicom_windows.lungs) show_image(px, figsize=(4,4)); . blurred = gauss_blur2d(px, 100) show_image(blurred, figsize=(4,4)); . #Todo results do not match up the chest area using the lungs dicom_window . show_image(blurred&gt;1); . Dataset.mask_from_blur . test = dimg.hist_scaled() blur = uniform_blur2d(dimg.hist_scaled(),50) ims = [test, blur] show_images(ims, titles=(&#39;orig&#39;, &#39;blurred&#39;), figsize=(10,10)) . dicom_windows.brain . (80, 40) . testblur = dimg.hist_scaled(min_px=0, max_px=100) blur = uniform_blur2d(dimg.hist_scaled(),50) test_mask = dimg.mask_from_blur(dicom_windows.brain, sigma=0.3, thresh=0.01, remove_max=True) ims = [testblur,test_mask] show_images(ims, titles=(&#39;orig&#39;, &#39;mask&#39;), figsize=(10,10)) . All values above +80 will be white and all values below 0 are black. . mask2bbox . #Todo . bbs = mask2bbox(test_mask) lo,hi = bbs print(bbs) print(lo[0], hi) . tensor([[ 0, 0], [1023, 1023]], device=&#39;cuda:0&#39;) tensor(0, device=&#39;cuda:0&#39;) tensor([1023, 1023], device=&#39;cuda:0&#39;) . crop_resize . #Todo . def _bbs2sizes(crops, init_sz, use_square=True): bb = crops.flip(1) print(f&#39;bb&quot; {bb} shape: {bb.shape}&#39;) szs = (bb[1]-bb[0]) print(f&#39;first szs: {szs} shape: {szs.shape}&#39;) if use_square: szs = szs.max(0)[0][None].repeat((2,1)) overs = (szs+bb[0])&gt;init_sz print(f&#39;overs: {overs}&#39;) bb[0][overs] = init_sz-szs[overs] lows = (bb[0]/float(init_sz)) return lows,szs/float(init_sz) . def crop_resize(x, crops, new_sz): # NB assumes square inputs. Not tested for non-square anythings! bs = x.shape[0] #number of channels print(bs) lows,szs = _bbs2sizes(crops, x.shape[-1]) print(f&#39;lows: {lows} n szs:{szs}&#39;) if not isinstance(new_sz,(list,tuple)): new_sz = (new_sz,new_sz) id_mat = tensor([[1.,0,0],[0,1,0]])[None].repeat((bs,1,1)).to(x.device) with warnings.catch_warnings(): warnings.filterwarnings(&#39;ignore&#39;, category=UserWarning) sp = F.affine_grid(id_mat, (bs,1,*new_sz))+1. grid = sp*unsqueeze(szs.t(),1,n=2)+unsqueeze(lows.t()*2.,1,n=2) return F.grid_sample(x.unsqueeze(1), grid-1) . Dataset.to_nchan . #Todo . show_images(dimg.to_nchan([dicom_windows.brain,dicom_windows.subdural,dicom_windows.abdomen_soft])) . show_images(dimg.to_nchan([dicom_windows.brain])) . Dataset.to_3chan . #Todo . Tensor.save_jpg . Save a tensor into a .jpg with specified windows . tensor_dicom = pixels(dimg) tensor_dicom.save_jpg(path=(pneumothorax_source/f&#39;train/01tensor.jpg&#39;), wins=[dicom_windows.lungs, dicom_windows.subdural]) . show_image(Image.open(pneumothorax_source/f&#39;train/01tensor.jpg&#39;)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b7b6ae7d48&gt; . Dataset.save_jpg . Save a DICOM image into a .jpg . dimg.save_jpg(path=(pneumothorax_source/f&#39;train/01dicom.jpg&#39;), wins=[dicom_windows.lungs, dicom_windows.lungs]) . show_image(Image.open(pneumothorax_source/f&#39;train/01dicom.jpg&#39;)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1cf8e996e48&gt; . Dataset.set_pixels . #Todo . Dataset.zoom . Zoom&#39;s image by specifying pixel ratio . dimg2 = dcmread(items[11]) dimg2.zoom(0.1) dimg2.show() . dimg2.zoom(0.5) dimg2.show() . Dataset.zoom_to . dimg2 = dcmread(items[11]) dimg2.shape, dimg2.show() . ((1024, 1024), None) . dimg2.zoom_to(90) dimg2.shape, dimg2.show() . ((90, 90), None) .",
            "url": "https://asvcode.github.io/Blogs/medical_imaging/dicom/fastai/2020/04/28/Medical-Imaging-Part2.html",
            "relUrl": "/medical_imaging/dicom/fastai/2020/04/28/Medical-Imaging-Part2.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Medical Imaging Part1",
            "content": "What are Dicom images? . DICOM(Digital Imaging and COmmunications in Medicine) is the de-facto standard that establishes rules that allow medical images(X-Ray, MRI, CT) and associated information to be exchanged between imaging equipment from different vendors, computers, and hospitals. The DICOM format provides a suitable means that meets health infomation exchange (HIE) standards for transmision of health related data among facilites and HL7 standards which is the messaging standard that enables clinical applications to exchange data. . DICOM files typically have a .dcm extension and provides a means of storing data in seperate &#39;tags&#39; such as patient information as well as image/pixel data. A DICOM file consists of a header and image data sets packed into a single file. The information within the header is organized as a constant and standardized series of tags. By extracting data from these tags one can access important information regarding the patient demographics, study parameters, etc . 16 bit DICOM images have values ranging from -32768 to 32768 while 8-bit greyscale images store values from 0 to 255. The value ranges in DICOM images are useful as they correlate with the Hounsfield Scale which is a quantitative scale for describing radiodensity . Parts of a DICOM Requirements . Requires installing pycidom . pip install pycidom | . and scikit-image . pip install scikit-image | . and kornia . pip install kornia | . Fastai provides an easy to access slim dicom dataset (250 DICOM files, ~30MB) from the SIIM-ACR Pneumothorax Segmentation dataset for us to experiment with dicom images. The file structure of the dataset is as follows: . from fastai2.basics import * from fastai2.callback.all import * from fastai2.vision.all import * from fastai2.medical.imaging import * import pydicom . #Load the Data pneumothorax_source = untar_data(URLs.SIIM_SMALL) . Patching . get_dicom_files . Provides a convenient way of recursively loading .dcm images from a folder. By default the folders option is set to False but you could specify a specific folder if required . #get dicom files items = get_dicom_files(pneumothorax_source, recurse=True, folders=&#39;train&#39;) items . (#250) [Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000000.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000002.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000005.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000006.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000007.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000008.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000009.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000011.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000012.dcm&#39;),Path(&#39;C:/Users/avird/.fastai/data/siim_small/train/No Pneumothorax/000014.dcm&#39;)...] . dcmread . Pydicom is a python package for parsing DICOM files and makes it easy to covert DICOM files into pythonic structures for easier manipulation. Files are opened using pydicom.dcmread . img = items[10] dimg = dcmread(img) type(dimg) . pydicom.dataset.FileDataset . You can now view all the information of the DICOM file. Explanation of each element is beyond the scope of this tutorial but this site has some excellent information about each of the entries. Information is listed by the DICOM tag (eg: 0008, 0005) or DICOM keyword (eg: Specific Character Set) . dimg . (0008, 0005) Specific Character Set CS: &#39;ISO_IR 100&#39; (0008, 0016) SOP Class UID UI: Secondary Capture Image Storage (0008, 0018) SOP Instance UID UI: 1.2.276.0.7230010.3.1.4.8323329.6340.1517875197.696624 (0008, 0020) Study Date DA: &#39;19010101&#39; (0008, 0030) Study Time TM: &#39;000000.00&#39; (0008, 0050) Accession Number SH: &#39;&#39; (0008, 0060) Modality CS: &#39;CR&#39; (0008, 0064) Conversion Type CS: &#39;WSD&#39; (0008, 0090) Referring Physician&#39;s Name PN: &#39;&#39; (0008, 103e) Series Description LO: &#39;view: AP&#39; (0010, 0010) Patient&#39;s Name PN: &#39;13f40bdc-803d-4fe0-b008-21234c2be1c3&#39; (0010, 0020) Patient ID LO: &#39;13f40bdc-803d-4fe0-b008-21234c2be1c3&#39; (0010, 0030) Patient&#39;s Birth Date DA: &#39;&#39; (0010, 0040) Patient&#39;s Sex CS: &#39;F&#39; (0010, 1010) Patient&#39;s Age AS: &#39;74&#39; (0018, 0015) Body Part Examined CS: &#39;CHEST&#39; (0018, 5101) View Position CS: &#39;AP&#39; (0020, 000d) Study Instance UID UI: 1.2.276.0.7230010.3.1.2.8323329.6340.1517875197.696623 (0020, 000e) Series Instance UID UI: 1.2.276.0.7230010.3.1.3.8323329.6340.1517875197.696622 (0020, 0010) Study ID SH: &#39;&#39; (0020, 0011) Series Number IS: &#34;1&#34; (0020, 0013) Instance Number IS: &#34;1&#34; (0020, 0020) Patient Orientation CS: &#39;&#39; (0028, 0002) Samples per Pixel US: 1 (0028, 0004) Photometric Interpretation CS: &#39;MONOCHROME2&#39; (0028, 0010) Rows US: 1024 (0028, 0011) Columns US: 1024 (0028, 0030) Pixel Spacing DS: [0.168, 0.168] (0028, 0100) Bits Allocated US: 8 (0028, 0101) Bits Stored US: 8 (0028, 0102) High Bit US: 7 (0028, 0103) Pixel Representation US: 0 (0028, 2110) Lossy Image Compression CS: &#39;01&#39; (0028, 2114) Lossy Image Compression Method CS: &#39;ISO_10918_1&#39; (7fe0, 0010) Pixel Data OB: Array of 118256 elements . Some key pointers on the tag information above: . Pixel Data (7fe0 0010) - This is where the raw pixel data is stored. The order of pixels encoded for each image plane is left to right, top to bottom, i.e., the upper left pixel (labeled 1,1) is encoded first | Photometric Interpretation (0028, 0004) - aka color space. In this case it is MONOCHROME2 where pixel data is represented as a single monochrome image plane where the minimum sample value is intended to be displayed as black info | Samples per Pixel (0028, 0002) - This should be 1 as this image is monochrome. This value would be 3 if the color space was RGB for example | Bits Stored (0028 0101) - Number of bits stored for each pixel sample | Pixel Represenation (0028 0103) - can either be unsigned(0) or signed(1). The default is unsigned. This Kaggle notebook by Jeremy explains why BitsStored and PixelRepresentation are important | Lossy Image Compression (0028 2110) - 00 image has not been subjected to lossy compression. 01 image has been subjected to lossy compression. | Lossy Image Compression Method (0028 2114) - states the type of lossy compression used (in this case JPEG Lossy Compression) | . Important tags not included in this dataset: . Rescale Intercept (0028, 1052) - The value b in relationship between stored values (SV) and the output units. Output units = m*SV + b. | Rescale Slope (0028, 1053) - m in the equation specified by Rescale Intercept (0028,1052). | . The Rescale Intercept and Rescale Slope are applied to transform the pixel values of the image into values that are meaningful to the application. Calculating the new values usually follow a linear formula: . NewValue = (RawPixelValue * RescaleSlope) + RescaleIntercept | . and when the relationship is not linear a LUT(LookUp Table) is utilized. . By default pydicom reads pixel data as the raw bytes found in the file and typically PixelData is often not immediately useful as data may be stored in a variety of different ways: . The pixel values may be signed or unsigned integers, or floats | There may be multiple image frames | There may be multiple planes per frame (i.e. RGB) and the order of the pixels may be different These are only a few examples and more information can be found on the pycidom website | . dimg.PixelData[:200] . b&#39; xfe xff x00 xe0 x00 x00 x00 x00 xfe xff x00 xe0 xe0 xcd x01 x00 xff xd8 xff xdb x00C x00 x03 x02 x02 x02 x02 x02 x03 x02 x02 x02 x03 x03 x03 x03 x04 x06 x04 x04 x04 x04 x04 x08 x06 x06 x05 x06 t x08 n n t x08 t t n x0c x0f x0c n x0b x0e x0b t t r x11 r x0e x0f x10 x10 x11 x10 n x0c x12 x13 x12 x10 x13 x0f x10 x10 x10 xff xc0 x00 x0b x08 x04 x00 x04 x00 x01 x01 x11 x00 xff xc4 x00 x1d x00 x00 x02 x02 x03 x01 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x00 x03 x04 x02 x05 x00 x01 x06 x07 x08 t xff xc4 x00] x10 x00 x01 x04 x01 x03 x02 x05 x01 x04 x05 x05 n x08 x0b x05 t x01 x00 x02 x03 x11 x04 x12!1 x05A x06 x13&#34;Qaq x142 x81 x91 x07#B xa1 xb1 x15R xc1 xd1 xd2 x08 x16$3b x92 x95 xb2 xb3 xe1%CSr x82 x93 xa2&#39; . Because of the complexity in interpreting PixelData, pydicom provides an easy way to get it in a convenient form: pixel_array which returns a numpy.ndarray containing the pixel data: . dimg.pixel_array, dimg.pixel_array.shape . (array([[ 2, 6, 5, ..., 3, 3, 2], [ 5, 9, 8, ..., 6, 5, 5], [ 5, 9, 9, ..., 6, 5, 5], ..., [ 49, 85, 80, ..., 123, 121, 69], [ 54, 88, 81, ..., 118, 115, 70], [ 17, 48, 39, ..., 46, 52, 27]], dtype=uint8), (1024, 1024)) . Class TensorDicom . Inherits from TensorImage . Class PILDicom . Inherits from PILBase . Opens a DICOM file from path fn or bytes fn and load it as a PIL Image. The DICOM is opened using pydicom.dcmread and accesses the pixel_array . type(PILDicom.create(img)) . fastai2.medical.imaging.PILDicom . pixels . Converts a pixel_array to a tensor . pixels(dimg) . tensor([[ 2., 6., 5., ..., 3., 3., 2.], [ 5., 9., 8., ..., 6., 5., 5.], [ 5., 9., 9., ..., 6., 5., 5.], ..., [ 49., 85., 80., ..., 123., 121., 69.], [ 54., 88., 81., ..., 118., 115., 70.], [ 17., 48., 39., ..., 46., 52., 27.]]) . scaled_pixel . pixels scaled by RescaleSlope and RescaleIntercept. The slim SIIM_SMALL dataset does not have RescaleSlope (0028,1053) or RescaleIntercept (0028,1052) in the dataset. . &gt;&gt; Side Note: Pixel Distribution . Having well scaled inputs is really important in getting good results from neural net training ref. This means having a normal or uniform distribution. The pixels in the DICOM image do not show a uniform distribution . px = dimg.pixels.flatten() plt.hist(px, bins=50); . In this case the image is showing multimodal distribution(having more than 2 peaks). Another case could be where the distribution is bimodal(having 2 distinct peaks). The functions below provide a means of splitting the range of pixel values into groups so that each group has an equal number of pixels . array_freqhist_bins . #Todo . Tensor.freqhist_bins . A function to split the range of pixel values into groups, such that each group has around the same number of pixels . Convert pydicom.dataset.FileDataset into a tensor . tensor_dicom = pixels(dimg) tensor_dicom . tensor([[ 2., 6., 5., ..., 3., 3., 2.], [ 5., 9., 8., ..., 6., 5., 5.], [ 5., 9., 9., ..., 6., 5., 5.], ..., [ 49., 85., 80., ..., 123., 121., 69.], [ 54., 88., 81., ..., 118., 115., 70.], [ 17., 48., 39., ..., 46., 52., 27.]]) . t_bin = tensor_dicom.freqhist_bins(n_bins=100) t_bin, t_bin.shape . (tensor([ 3., 4., 5., 6., 7., 8., 9., 10., 11., 13., 16., 21., 26., 31., 38., 45., 52., 57., 62., 65., 68., 70., 72., 74., 76., 78., 79., 81., 82., 83., 84., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97., 98., 99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109., 110., 111., 112., 113., 115., 116., 117., 119., 120., 122., 123., 124., 126., 127., 129., 131., 132., 134., 136., 138., 140., 143., 145., 148., 151., 155., 158., 161., 163., 165., 167., 169., 171., 173., 175., 177., 179., 180., 182., 184., 186., 189., 192., 197., 203.]), torch.Size([100])) . freqhist_bins splits the pixels into bins, the number of bins set by n_bins. So for the above example: . freqhist_bins flattens out the image tensor (in this case 1024 by 1024 into a flattened tensor of size 1048576 (1024*1024) | setting n_bins to 1 for example means it will be split into 3 bins (the beginning, the end and the number of bins specified by n_bins | each bin is then scaled to values between 0 and 255 (in this case the bulk of pixels are grouped at 3, 103 and 203 | . with a n_bin of 1 . plt.hist(t_bin, bins=t_bin); plt.plot(t_bin, torch.linspace(0,1,len(t_bin)));dimg.show(t_bin) . with n_bins of 100 the histogram displays that each group has the same number of pixels and you can also notice that the quality of the image is different compared to the image above . plt.hist(t_bin, bins=t_bin); plt.plot(t_bin, torch.linspace(0,1,len(t_bin)));dimg.show(t_bin) . Tensor.hist_scale . A way to scale a tensor of pixels evenly using freqhist_bins to values between 0 and 1. . tensor_dicom . tensor([[ 2., 6., 5., ..., 3., 3., 2.], [ 5., 9., 8., ..., 6., 5., 5.], [ 5., 9., 9., ..., 6., 5., 5.], ..., [ 49., 85., 80., ..., 123., 121., 69.], [ 54., 88., 81., ..., 118., 115., 70.], [ 17., 48., 39., ..., 46., 52., 27.]]) . The tensor has values between 0 and 255 . #Run to view - commented out to reduce file size #plt.hist(tensor_dicom, bins=100) . The above commented out code displays a histogram of pixel values which range from 0 to 255 . Using hist_scaled gets values now scaled between 0 and 1 . tensor_hists = tensor_dicom.hist_scaled() tensor_hists . tensor([[0.0000, 0.0303, 0.0202, ..., 0.0000, 0.0000, 0.0000], [0.0202, 0.0606, 0.0505, ..., 0.0303, 0.0202, 0.0202], [0.0202, 0.0606, 0.0606, ..., 0.0303, 0.0202, 0.0202], ..., [0.1573, 0.3081, 0.2677, ..., 0.6566, 0.6414, 0.2071], [0.1657, 0.3333, 0.2727, ..., 0.6212, 0.5960, 0.2121], [0.1030, 0.1558, 0.1429, ..., 0.1530, 0.1616, 0.1232]]) . #plotting the scaled histogram #plt.hist(tensor_hists, bins=100) . Scaled histogram now has pixel values ranging from 0 to 1 . Dataset.hist_scaled . dimg.pixel_array . array([[ 2, 6, 5, ..., 3, 3, 2], [ 5, 9, 8, ..., 6, 5, 5], [ 5, 9, 9, ..., 6, 5, 5], ..., [ 49, 85, 80, ..., 123, 121, 69], [ 54, 88, 81, ..., 118, 115, 70], [ 17, 48, 39, ..., 46, 52, 27]], dtype=uint8) . data_scaled = dimg.hist_scaled(min_px=1, max_px=10) data_scaled . tensor([[0.0000, 0.4286, 0.2857, ..., 0.0000, 0.0000, 0.0000], [0.2857, 0.8571, 0.7143, ..., 0.4286, 0.2857, 0.2857], [0.2857, 0.8571, 0.8571, ..., 0.4286, 0.2857, 0.2857], ..., [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000]]) . #max_px = 10 plt.imshow(data_scaled, cmap=plt.cm.bone); . #max_px = 100 data_scaled2 = dimg.hist_scaled(min_px=1, max_px=100) plt.imshow(data_scaled2, cmap=plt.cm.bone) . &lt;matplotlib.image.AxesImage at 0x2472852ec48&gt; . &gt;&gt; Side Note: Windowing . DICOM images can contain a high amount of voxel values and windowing can be thought of as a means of manipulating these values in order to change the apperance of the image so particular structures are highlighted. A window has 2 values: . l = window level or center aka brightness . w = window width or range aka contrast . Example: from here . Brain Matter window . l = 40 (window center) w = 80 (window width) . Voxels displayed range from 0 to 80 . Calculating voxel values: . lowest_visible_value = window_center - window_width / 2 | highest_visible_value = window_center + window_width / 2 | . (lowest_visible_value = 40 - (80/2), highest_visible_value = 40 + (80/2)) . Hence all values above &gt;80 will be white and all values below 0 are black. . Dataset.windowed . Takes 2 values w and l . fastai conveniently provides a range of window width and centers (dicom_windows) for viewing common body areas: . brain=(80,40), subdural=(254,100), stroke=(8,32), brain_bone=(2800,600), brain_soft=(375,40), lungs=(1500,-600), mediastinum=(350,50), abdomen_soft=(400,50), liver=(150,30), spine_soft=(250,50), spine_bone=(1800,400) . plt.imshow(dimg.windowed(w=1500, l=-600), cmap=plt.cm.bone) . &lt;matplotlib.image.AxesImage at 0x247280734c8&gt; . plt.imshow(dimg.windowed(*dicom_windows.lungs), cmap=plt.cm.bone) . &lt;matplotlib.image.AxesImage at 0x24727ae4ec8&gt; . Example . img = items[10] dimg = dcmread(img) . Convert a DICOM image into tensors . tensor_dicom = pixels(dimg) tensor_dicom . tensor([[ 2., 6., 5., ..., 3., 3., 2.], [ 5., 9., 8., ..., 6., 5., 5.], [ 5., 9., 9., ..., 6., 5., 5.], ..., [ 49., 85., 80., ..., 123., 121., 69.], [ 54., 88., 81., ..., 118., 115., 70.], [ 17., 48., 39., ..., 46., 52., 27.]]) . View a portion of the image . e = tensor(tensor_dicom)[200:440,600:840] e.shape . torch.Size([240, 240]) . df = pd.DataFrame(e) #commented out conserve memory #df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . The dataframe created by df looks like this . dicom_windows = types.SimpleNamespace( test=(50,60) ) . r = e.windowed(w=50, l=60) . #This will produce a Dataframe of each pixel whether &#39;true&#39; or &#39;false&#39; #commented out due to high memory use test_e = e.mask_from_blur(dicom_windows.test, thresh=0.05, remove_max=True) #dfe = pd.DataFrame(test_e) #dfe.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . imk = [e,r, test_e] show_images(imk, titles=(&#39;orig&#39;, &#39;windowed&#39;, &#39;mask&#39;), figsize=(15,15)) . using show_images the images appear green because show_images and show_image by default use the &#39;viridis&#39; colormap and PILBase follows the same default colormap. Its important to note that the colormap choice is for the human benefit (it does not actually affect model training). However looking at the image above it looks very un-natural and hence better to change to another colormap choice. The &#39;bone&#39; colormap displays images with bone colors (which is what show uses) . matplotlib.rcParams[&#39;image.cmap&#39;] = &#39;bone&#39; . imk = [e,r, test_e] show_images(imk, titles=(&#39;orig&#39;, &#39;windowed&#39;, &#39;mask&#39;), figsize=(15,15)) . show . Inherits from show_image and is customized for displaying DICOM images . dimg.show(scale=True, cmap=plt.cm.bone, min_px=-1100, max_px=None, ax=None, figsize=(6,6), title=&#39;Test&#39;, ctx=None) . Jeremy mentioned in this article about using a &#39;rainbow colormap&#39; to fully utilize our computer&#39;s ability to display color . dimg.show(cmap=plt.cm.gist_ncar, figsize=(6,6)) . Continue to Part2 &gt;&gt;&gt;&gt; .",
            "url": "https://asvcode.github.io/Blogs/medical_imaging/dicom/fastai/2020/04/28/Medical-Imaging-Part1.html",
            "relUrl": "/medical_imaging/dicom/fastai/2020/04/28/Medical-Imaging-Part1.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastai2 Image Augmentation",
            "content": "Intro . Fastai2 provides a wide range of data augmentation techniques and this blog is particularly focused on image augmentation techniques (This is a update to the article &#39;Data Augmentation Techniques&#39; I wrote in 2018 using fastai v1[1]) . Working with limited data has its own challenges, using data augmentation can have positive results only if the augmentation techniques enhance the current data set for example is there any worth is training a network to ‘learn’ about a landmark in a flipped upside down orientation? . Invariance is the ability of convolutional neural networks to classify objects even when they are placed in different orientations. Data augmentation is a way of creating new ‘data’ with different orientations. The benefits of this are two fold, the first being the ability to generate ‘more data’ from limited data and secondly it prevents over fitting. . Most deep learning libraries use a step by step method of augmentation whilst *fastai2 utilizes methods that combine various augmentation parameters to reduce the number of computations and reduce the number of lossy operations*[2]. . Fastai uses Pipelines to compose several transforms together. A Pipeline is defined by passing a list of Transforms and it will then compose the transforms inside it. In this blog I will look at what order these transforms are conducted and what effect they have on image quality and efficiency. Pipelines are sorted by the internal order atribute (more discussed below) with a default order of 0. . Using this as a high-level API example . from fastai2.vision.all import* source = untar_data(URLs.PETS) . #High-level API example testblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=[Resize(256)], batch_tfms=[*aug_transforms(xtra_tfms=None)]) test_dls = testblock.dataloaders(source/&#39;images&#39;) . test_dls.show_batch(max_n=6, nrows=1, ncols=6) . To check the order of how augementations are conducted we can call *after_item* and *after_batch* . after_item . test_dls.after_item . Pipeline: Resize -&gt; ToTensor . In this case images are: . resized to sizes of equal length, in this case 256 and then . convert the image into a *channel* X *height* X *weigth* tensor . But what does Resize do? . Click the button to view the Resize class . #collapse #https://github.com/fastai/fastai2/blob/master/nbs/09_vision.augment.ipynb @delegates() class Resize(RandTransform): split_idx = None mode,mode_mask,order,final_size = Image.BILINEAR,Image.NEAREST,1,None &quot;Resize image to `size` using `method`&quot; def __init__(self, size, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection, resamples=(Image.BILINEAR, Image.NEAREST), **kwargs): super().__init__(**kwargs) self.size,self.pad_mode,self.method = _process_sz(size),pad_mode,method self.mode,self.mode_mask = resamples def before_call(self, b, split_idx): if self.method==ResizeMethod.Squish: return self.pcts = (0.5,0.5) if split_idx else (random.random(),random.random()) def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)): orig_sz = _get_sz(x) self.final_size = self.size if self.method==ResizeMethod.Squish: return x.crop_pad(orig_sz, Tuple(0,0), orig_sz=orig_sz, pad_mode=self.pad_mode, resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size) w,h = orig_sz op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad] m = w/self.size[0] if op(w/self.size[0],h/self.size[1]) else h/self.size[1] cp_sz = (int(m*self.size[0]),int(m*self.size[1])) tl = Tuple(int(self.pcts[0]*(w-cp_sz[0])), int(self.pcts[1]*(h-cp_sz[1]))) return x.crop_pad(cp_sz, tl, orig_sz=orig_sz, pad_mode=self.pad_mode, resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size) . . By default resize &#39;squishes&#39; the image to the size specified. The image is resized so that the shorter dimension matches the size specifed and the rest padded with what is specified in pad_mode. . The method parameter can be be 1 of 3 values: Crop, Pad or Squish(default) eg: *method=ResizeMethod.Squish* The padding parameter also takes 1 of 3 values: Border, Zeros and Reflection(default) eg: *pad_mode=PadMode.Reflection*. . The images are resized/resamples using bilinear and nearest neighbour interprolations[3]. . We can check to see how initial image sizes are affected by Resize. I choose an image with numbers so that you can see different areas of the image easier and I colored each of the corners a different color to better see what effects Resize has on the image. . #Load a test image image_path = &#39;C:/Users/avird/.fastai/data/0100-number_12.jpg&#39; img = Image.open(image_path) img.shape, type(img) . ((380, 500), PIL.JpegImagePlugin.JpegImageFile) . #Convert image into a fastai.PILImage img = PILImage(PILImage.create(image_path).resize((500,380))) img.shape, type(img) . ((380, 500), fastai2.vision.core.PILImage) . #View the image img . Fastai uses 3 types of resize methods (using *ResizeMethod*: Squish, Pad and Crop) and they can be plotted against each other to view the differences between them. Squish is the fastai default. To better view the differences I used a padding of zeros.(the default for padding is Reflection) . &gt; Image size 5 . #collapse #Use image size of 5 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(5, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=1), ctx=ax, title=f&#39;{method}, size=5&#39;); . . Using an image size of 5 we can see how the image is affected by Resize. At this size we can see all the 4 different colors in each corner and there is not much difference between squish abd crop. With pad however the image is being resized so the shorter dimension (in this case the height (as the original image size is 380 height and 500 width) is matched to the image size of 256 and then padded with zeros. . &gt; Image size 15 . #collapse #Use image size of 15 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(15, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=1), ctx=ax, title=f&#39;{method}, size=15&#39;); . . At image size 15, both &#39;squish&#39; and &#39;pad&#39; are still showing all the colors in the corners but with &#39;crop&#39; you start to notice that the colors in each corner are begining to fade as the image is being cropped from the center . &gt; Image size 256 . #collapse #Use image size of 256 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(256, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=0), ctx=ax, title=f&#39;{method}, size=256&#39;); . . At 256 both &#39;squish&#39; and &#39;pad&#39; still display the full image and &#39;crop&#39; displays the cropped image . What impacts could this have on real datasets. . Using an image from a Covid19 dataset [5] . #collapse test_path = &#39;C:/Users/avird/.fastai/data/0002.jpeg&#39; testimg = Image.open(test_path) img2 = PILImage(PILImage.create(test_path).resize((944, 656))) img2 . . #collapse #Use image size of 256 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(256, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img2, split_idx=1), ctx=ax, title=f&#39;{method}, size=256&#39;); . . In this case: . the default &#39;squish&#39; resize method squishes the image on the horizontal axis. You can view the whole image however you can see that ribcage has been constricted towards the center. The implications of this could mean that important features you see in the original image could either be erased or diluted. . for the &#39;pad&#39; resize the image is still viewable fully but again the image has been squished on the vertical axis. . With &#39;crop&#39;, the image is cropped from the centre hence we lose image details from the edges . The implications of these choices is really dependant on the dataset but they could have an detrimental effect if the wrong choice is choosen leading to vital features being erased or diluted . after_batch . Back to the pets example if we run *after_batch*, this shows us the after batch augmentation pipeline. Previously item_tfms is used to resize the images and to collate them into tensors ready for GPU processing. . test_dls.after_batch . Pipeline: IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm . This reveals the pipeline process for the batch transformations: . convert ints to float tensors . apply all the affine transformations . followed by the lighting transformations . The order is important in order to maintain a number of key aspects:&gt; Maintain image quality . Reduce computations . Improve efficiency . As mentioned in Fastbook [4], most machine libraries use a step by step process of augmentation which can lead to a reduced quality of images. The datablock example above is an example of a high-level API which is pretty flexible but not as much as a mid-level API. . The mid-level datablock below is an exact example of the high-level datablock above and allows for more customizations and we will use this datablock for the rest of the blog . #collapse #Helper for viewing single images def repeat_one(source, n=128): &quot;&quot;&quot;Single image helper for displaying batch&quot;&quot;&quot; return [get_image_files(source)[1]]*n . . Mid-Level API and viewing a batch of a single image . #mid-level API example #num_workers = 0 because I use windows :) and windows does not support multiprocessing on CUDA [6] tfms = [[PILImage.create], [using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), Categorize]] item_tfms = [ToTensor(), Resize(256)] splitter=RandomSplitter(seed=42) after_b = [IntToFloatTensor(), *aug_transforms(xtra_tfms=RandomResizedCrop(256), min_scale=0.9)] dsets = Datasets(repeat_one(source/&#39;images&#39;), tfms=tfms) dls = dsets.dataloaders(after_item=item_tfms, after_batch=after_b, bs=32, num_workers=0, splits=splitter) . dls.after_batch . Pipeline: RandomResizedCrop -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm . #collapse dls.show_batch(max_n=6, nrows=1, ncols=6) . . Image comparisons (Fastai v The Rest) . #create 1 batch x,y = dls.one_batch() . Checking image quality and speed using step by step transformations. . #collapse time x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=256) x1 = x1.brightness(max_lighting=0.2, p=1.) x1 = x1.zoom(max_zoom=1.1, p=0.5) x1 = x1.warp(magnitude=0.2, p=0.5) _,axs = subplots(1, 1, figsize=(5,5)) TensorImage(x1[0]).show(ctx=axs[0]) . . Wall time: 103 ms . &lt;matplotlib.axes._subplots.AxesSubplot at 0x29421ecdfc8&gt; . Checking image quality and speed using fastai2 . #collapse %%time tfms = setup_aug_tfms([Brightness(max_lighting=0.2, p=1.,), CropPad(size=256), Zoom(max_zoom=1.1, p=0.5), Warp(magnitude=0.2, p=0.5) ]) x = Pipeline(tfms)(x) _,axs = subplots(1, 1, figsize=(5,5)) TensorImage(x[0]).show(ctx=axs[0]) . . Wall time: 45.9 ms . &lt;matplotlib.axes._subplots.AxesSubplot at 0x294220787c8&gt; . Comparing the times above using a pipeline where a list of transforms are passed in is nearly twice as fast as using augmentations step by step. In this case the step by step method completed the task in 103ms compard to 46s using fastai . Looking at side by side look at image quality . #collapse image_comp(): x,y = dls.one_batch() tfms = setup_aug_tfms([Brightness(max_lighting=0.3, p=1.,), Resize(size=256), Zoom(max_zoom=1.1, p=1.), Warp(magnitude=0.2, p=1.) ]) x = Pipeline(tfms)(x) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=256) x1 = x1.brightness(max_lighting=0.3, p=1.) x1 = x1.zoom(max_zoom=1.1, p=1.) x1 = x1.warp(magnitude=0.2, p=1.) _,axs = subplots(1, 2, figsize=(20,20)) TensorImage(x[0]).show(ctx=axs[0], title=&#39;fastai&#39;) TensorImage(x1[0]).show(ctx=axs[1], title=&#39;other&#39;) . . image_comp() . You can definately see differences between the two pictures, the &#39;fastai&#39; image is more clearer compared to the &#39;other&#39; image. How about some other examples . image_comp() . image_comp() . List of Transforms . There a number of transforms and here is a list of the most common ones . RandomResizedCrop = &quot;Picks a random scaled crop of an image and resize it to size - order 1&quot; IntToFloatTensor = &quot;Transform image to float tensor, optionally dividing by 255 (e.g. for images) - order 10 Rotate = &quot;Apply a random rotation of at most max_deg with probability p to a batch of images&quot; Brightness = &quot;Apply change in brightness of max_lighting to batch of images with probability p.&quot; RandomErasing = &quot;Randomly selects a rectangle region in an image and randomizes its pixels.&quot; - order 100 CropPad = &quot;Center crop or pad an image to size&quot; - order 0 Zoom = &quot;Apply a random zoom of at most max_zoom with probability p to a batch of images&quot; Warp = &quot;Apply perspective warping with magnitude and p on a batch of matrices&quot; Contrast = &quot;Apply change in contrast of max_lighting to batch of images with probability p.&quot; . Pipeline for multiple augmentations . In the example above the after_batch pipeline consisted of IntToFloatTensor &gt; Affine tranformations &gt; Lighting transformations. . However what we uses additional augmentations, what does the pipeline look like then? . #collapse source = untar_data(URLs.PETS) #Helper for viewing single images def repeat_one(source, n=128): &quot;&quot;&quot;Single image helper for displaying batch&quot;&quot;&quot; return [get_image_files(source)[2]]*n . . #collapse #Include multiple transforms tfms = [[PILImage.create], [using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), Categorize]] item_tfms = [ToTensor(), Resize(296)] splitter=RandomSplitter(seed=42) xtra_tfms = [Rotate(max_deg=45, p=1.), RandomErasing(p=1., max_count=10, min_aspect=0.5, sl=0.2, sh=0.2), RandomResizedCrop(p=1., size=256), Brightness(max_lighting=0.2, p=1.), CropPad(size=256), Zoom(max_zoom=2.1, p=0.5), Warp(magnitude=0.2, p=1.0) ] after_b = [IntToFloatTensor(), *aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=0., max_zoom=1.1, max_lighting=0.,max_warp=0., p_affine=0.75, p_lighting=0.75, xtra_tfms=xtra_tfms, size=256, mode=&#39;bilinear&#39;, pad_mode=PadMode.Reflection, align_corners=True, batch=False, min_scale=0.9)] mdsets = Datasets(repeat_one(source/&#39;images&#39;), tfms=tfms) mdls = mdsets.dataloaders(after_item=item_tfms, after_batch=after_b, bs=32, num_workers=0, splits=splitter) . . Looking at after_item - it is the same as before . mdls.after_item . Pipeline: Resize -&gt; ToTensor . mdls.after_batch . Pipeline: CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm -&gt; RandomErasing . after_batch is now a different story and we can see the list of how fastai computes its augmentations. These are all done in sequence (depending on their order) starting with . CropPad . followed by affine . lighting . and random erasing transforms. . Here is what the batch looks like . mdls.show_batch(max_n=6, nrows=1, ncols=6) . The order number determines the sequence of the transforms for example CropPad is order 0, Resize and RandomCrop are order 1 hence the reason they appear first on the list. IntToFloatTensor is order 10 and runs after PIL transforms on the GPU. Affine transforms are order 30 and so is RandomResizedCropGPU and lighting transforms are order 40. RandomErasing is order 100. . Viewing the order of transforms . #for example mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . RandomResizedCrop.order, CropPad.order, IntToFloatTensor.order, AffineCoordTfm.order, RandomResizedCropGPU.order, RandomErasing.order . (0, 0, 10, 30, 30, 100) . You can force the order by implicity specifying the order of a transform by stating the order within a transform class. . Interesting Observations . There were some interesting observations during this experimention. Adding a *min_scale* value in aug_transforms adds RandomResizedCropGPU to the pipeline . mdls.after_batch . Pipeline: CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . However if you add RandomResizedCrop as well as a min_scale value the pipeline now looks like this . mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . And if you use RandomResizedCrop with no min_scale value the pipeline is now: . mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm -&gt; RandomErasing . Still to do . There is clearly a plethora of options and additonal experimentation is needed to see what the impact of the various pipelines are on image quality, efficiency and end results -*work in progress* . Manually going through the pipeline . Attempt to manually go throught the pipeline. . #collapse image_path = &#39;C:/Users/avird/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39; TEST_IMAGE = Image.open(image_path) img = PILImage(PILImage.create(image_path)) img.shape, type(img) . . ((400, 600), fastai2.vision.core.PILImage) . This is the original image of size 400 height and 600 width . #collapse #Original Image img . . Resize to 256 using default crop and reflection padding . #collapse #Resize to 256 using default crop and reflection padding r = Resize(256, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection) w = r(img) w.shape, type(w) . . ((256, 256), fastai2.vision.core.PILImage) . #collapse w . . Crop the image using size 256 . #collapse Crop crp = CropPad(256) c = r(crp(img)) c.shape, type(c) . . ((256, 256), fastai2.vision.core.PILImage) . #collapse c . . Convert PILImage into a TensorImage . timg = TensorImage(array(c)).permute(2,0,1).float()/255. timg.shape, type(timg) . (torch.Size([3, 256, 256]), fastai2.torch_core.TensorImage) . h = TensorImage(timg[None].expand(3, *timg.shape).clone()) h.shape, type(h) . (torch.Size([3, 3, 256, 256]), fastai2.torch_core.TensorImage) . #if do_flip=true and flip-vert=false = Flip fli = Flip(p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y = fli(h) show_image(y[0], ctx=ax, cmap=&#39;Greys&#39;) . #if do_flp=true and flip_vert=true = dihyderal dih = Dihedral(p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y1 = dih(h) show_image(y1[0], ctx=ax) . #Rotate rot = Rotate(max_deg=45, p=1.) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y2 = rot(h) show_image(y2[0], ctx=ax) . # Zoom zoo = Zoom(max_zoom=4.1, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y3 = zoo(h) show_image(y3[0], ctx=ax) . # Warp war = Warp(magnitude=0.7, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y4 = war(h) show_image(y4[0], ctx=ax) . #Brightness bri = h.brightness(draw=0.9, p=1.) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y5 = bri show_image(y5[0], ctx=ax) . #Contrast con = h.contrast(draw=1.9, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y6 = con show_image(y6[0], ctx=ax) . View the images side by side . #collapse _,axs = plt.subplots(1,8, figsize=(20,9)) for i,ax in enumerate(axs.flatten()): y7 = y1 + y4 show_image(img, ctx=axs[0], title=&#39;original&#39;) show_image(w, ctx=axs[1], title=&#39;resize 256&#39;) show_image(y[0], ctx=axs[2], title=&#39;flip&#39;) show_image(y2[0], ctx=axs[3], title=&#39;rotate&#39;) show_image(y3[0], ctx=axs[4], title=&#39;zoom&#39;) show_image(y4[0], ctx=axs[5], title=&#39;warp&#39;) show_image(y5[0], ctx=axs[6], title=&#39;brighness&#39;) show_image(y6[0], ctx=axs[7], title=&#39;contrast&#39;) . . References: . 1: https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b . 2: https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb . 3: https://github.com/fastai/fastai2/blob/master/nbs/09_vision.augment.ipyn . 4: https://github.com/fastai/fastbook . 5: https://github.com/lindawangg/COVID-Net . 6: https://forums.fast.ai/t/windows-runtimeerror-cuda-runtime-error-801-runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type/57333 .",
            "url": "https://asvcode.github.io/Blogs/fastai/augmentation/image-augmentation/2020/03/26/Fastai2-Image-Augmentation.html",
            "relUrl": "/fastai/augmentation/image-augmentation/2020/03/26/Fastai2-Image-Augmentation.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Covid 19 Jamii Yako Na Wewe Mtazamo Wa Sayansi Ya Data",
            "content": "Covid-19, jamii yako, na wewe - mtazamo wa sayansi ya data . This is the Swahili translation of the article written by Jeremy Howard and Rachel Thomas Covid-19, your community, and you — a data science perspective . Imeandikwa: 09 Mar 2020 by Jeremy Howard and Rachel Thomas . Ilitafsiriwa na Amrit Virdee . Sisi ni wanasayansi wa data-Hiyo ni, kazi yetu ni kuelewa jinsi ya kuchambua na kutafsiri data. Wakati tunachambua data karibu covid-19, tunajali sana. Sehemu zilizo hatarini zaidi za jamii, wazee na masikini, ziko hatarini zaidi, lakini kudhibiti kuenea na athari za ugonjwa kunahitaji sisi sote kubadili tabia yetu. Osha mikono yako vizuri na mara kwa mara, epuka vikundi na umati wa watu, futa matukio, na usiguse uso wako. Katika chapisho hili, tunaelezea kwa nini tunajali, na unapaswa kuwa pia. Kwa muhtasari bora wa habari muhimu unayohitaji kujua, soma Corona kwa kifupi na Ethan Alley (rais wa mashirika yasiyo ya faida ambayo huendeleza teknolojia za kupunguza hatari kutoka kwa magonjwa ya milipuko). . Tafsiri . Mtu yeyote anakaribishwa kutafsiri kifungu hiki, kusaidia jamii zao za mitaa kuelewa maswala haya. Tafadhali unganisha hapa na mkopo unaofaa. Tujulishe kwenye Twitter ili tuweze kuongeza tafsiri yako kwenye orodha hii. . French . Spanish . German . Portuguese (Brazil) . Chinese 中文简体 . Thai . Tunahitaji mfumo wa matibabu unaofanya kazi . Zaidi ya miaka 2 iliyopita mmoja wetu (Rachel) alipata maambukizi ya ubongo ambayo huua karibu 1/4 ya watu wanaopata, na huacha 1/3 akiwa na udhaifu wa kudumu wa utambuzi. Wengine wengi huishia na maono ya kudumu na uharibifu wa kusikia. Rachel alikuwa mwepesi wakati wa kutambaa katika maegesho ya hospitali. Alikuwa na bahati ya kutosha kupata utunzaji wa haraka, utambuzi, na matibabu. Hadi muda mfupi kabla ya tukio hili Rachel alikuwa kwenye afya njema. Kuwa na ufikiaji wa haraka katika chumba cha dharura hakika kuliokoa maisha yake. . Sasa, wacha tuzungumze juu ya covid-19, na nini kinaweza kutokea kwa watu walio katika hali ya Rachel katika wiki na miezi ijayo. Idadi ya watu waliopatikana wameambukizwa covid-19 mara mbili kila siku 3 hadi 6. Pamoja na kiwango cha kuongezeka kwa siku tatu, hiyo inamaanisha kwamba idadi ya watu waliopatikana wameambukizwa inaweza kuongezeka mara 100 katika wiki tatu (sio rahisi sana, lakini wasikatishwe na maelezo ya kiufundi). Mtu mmoja kati ya 10 aliyeambukizwa anahitaji kulazwa hospitalini kwa wiki nyingi, na nyingi hizi zinahitaji oksijeni. Ingawa ni siku za mapema sana kwa virusi hivi, tayari kuna mikoa ambayo hospitali zimezidiwa kabisa, na watu hawana uwezo wa kupata matibabu wanayohitaji (sio tu kwa covid-19, bali pia kwa kitu kingine chochote, kama vile huduma ya kuokoa maisha ambayo Rachel alihitaji). Kwa mfano, nchini Italia, ambapo wiki moja tu maafisa walikuwa wakisema kwamba kila kitu kiko sawa, sasa watu milioni kumi na sita wamewekwa chini (sasisha: masaa 6 baada ya kuchapisha hii, Italia iliiweka nchi nzima chini, na mahema kama haya yanawekwa kusaidia kushughulikia kuongezeka kwa wagonjwa: . Hema la matibabu linalotumika huko Italia . . Dr Antonio Pesenti, mkuu wa kitengo cha kukabiliana na msiba wa mkoa katika eneo lililo ngumu sana nchini Italia, alisema, “Sasa tunalazimishwa kuanzisha matibabu ya matunzo makubwa katika barabara, katika uwanja wa michezo, katika vyumba vya uokoaji … Moja ya mifumo bora ya afya ulimwenguni, huko Lombardy ni hatua mbali na kuanguka. “. . Hii sio kama mafua . Homa hiyo ina kiwango cha vifo takriban 0.1% ya maambukizo. Marc Lipsitch, mkurugenzi wa Kituo cha Dalili za Magonjwa ya Kuambukiza huko Harvard, anakadiria kuwa kwa covid-19 ni 1-2%. Mitindo ya hivi karibuni ya epedemiological ilipata kiwango cha 1.6% nchini Uchina mnamo Februari, mara kumi na sita zaidi kuliko flu[1] (hii inaweza kuwa nambari ya kihafidhina, kwa sababu viwango vinapanda sana wakati mfumo wa matibabu hauwezi kuhimili). Makadirio ya sasa bora yanatarajia kwamba covid-19 ataua watu zaidi ya mara 10 mwaka huu kuliko homa hiyo (na kuigwa na Elena Grewal, mkurugenzi wa zamani wa sayansi ya data huko Airbnb, inaonyesha inaweza kuwa mara 100 zaidi, katika hali mbaya zaidi). Hii ni kabla ya kuzingatia athari kubwa kwenye mfumo wa matibabu, kama ile ilivyoelezwa hapo juu. Inafahamika kuwa watu wengine wanajaribu kujishawishi kwamba hii sio jambo jipya, ugonjwa kama mafua, kwa sababu sio vizuri sana kukubali ukweli kwamba hii sio kawaida kabisa. . Kujaribu kuelewa intuitively ukuaji wa kuongezeka kwa idadi ya watu walioambukizwa sio jambo ambalo akili zetu zimetengenezwa kushughulikia. Kwa hivyo inabidi tuchunguze hii kama wanasayansi, sio kutumia ubuni wetu. . Hii itakuwa wapi katika wiki 2? Miezi 2? . . Kwa kila mtu ambaye ana mafua, kwa wastani, huwaambukiza watu wengine 1.3. Hiyo inaitwa “R0” kwa homa. Ikiwa R0 ni chini ya 1.0, basi maambukizi huacha kuenea na kufa nje. Ikiwa ni zaidi ya 1.0, inaenea. R0 kwa sasa ni 2-3 kwa covid-19 nje ya Uchina. Tofauti inaweza kuonekana kuwa ndogo, lakini baada ya “vizazi” 20 vya watu walioambukizwa kupitisha maambukizo yao, R0 ya 1.3 inaweza kusababisha maambukizo 146, lakini R0 ya 2.5 inaweza kusababisha maambukizi milioni 36! (Hii ni kweli, ni ya mkono sana na inapuuza athari nyingi za ulimwengu, lakini ni mfano mzuri wa tofauti kati ya covid-19 na mafua, vitu vingine vyote kuwa sawa). . Kumbuka kuwa R0 sio mali ya msingi ya ugonjwa. Inategemea sana majibu, na inaweza kubadilika kwa wakati[2]. Kwa kushangaza zaidi, nchini China R0 kwa covid-19 imeshuka sana, na sasa inakaribia 1.0! Jinsi, unauliza? Kwa kuweka vipimo kwa kiwango ambacho itakuwa ngumu kufikiria katika nchi kama Amerika - kwa mfano, kufunga kabisa miji mikuu, na kuendeleza mchakato wa upimaji unaoruhusu watu zaidi ya milioni kwa wiki kupimwa. . Jambo moja ambalo huja kwenye media ya kijamii (pamoja na akaunti zilizofuatwa sana kama Elon Musk) ni kutokuelewana kwa tofauti kati ya mantiki na ufafanuzi ukuaji. Ukuaji wa mantiki unataja mfano wa ukuaji wa “umbo” la janga lililoenea katika mazoezi. Ni wazi ukuaji wa ukuaji hauwezi kuendelea milele, kwani sivyo kungekuwa na watu wengi walioambukizwa kuliko watu ulimwenguni! Kwa hivyo, mwishowe, viwango vya maambukizi lazima vimepungua kila wakati, na kusababisha kiwango cha ukuaji (kinachojulikana kama sigmoid) kwa wakati. Walakini, ukuaji unaopungua hujitokeza tu kwa sababu-sio uchawi. Sababu kuu ni: . Jibu kubwa na linalofaa la jamii, au. . | Asilimia kubwa kama hiyo ya watu wameambukizwa kwamba kuna watu wachache ambao hawajaambukizwa waeneze. . | . Kwa hivyo, haifanyi akili mantiki kutegemea muundo wa ukuaji wa vifaa kama njia ya “kudhibiti” janga. . Jambo lingine ambayo inafanya kuwa ngumu kuelewa athari za covid-19 katika jamii yako ni kwamba kuna kuchelewesha sana kati ya maambukizo na kulazwa hospitalini - kwa ujumla karibu siku 11. Hii inaweza kutoonekana kama muda mrefu, lakini ukilinganisha na idadi ya watu walioambukizwa wakati huo, inamaanisha kuwa kwa wakati utagundua kwamba vitanda vya hospitali vimejaa, maambukizi ya jamii tayari yapo katika kiwango ambacho kutakuwa na Mara 5-10 watu zaidi wa kushughulika nao. . Kumbuka kuwa kuna ishara kadhaa za mapema kuwa athari katika eneo lako inaweza angalau kutegemea hali ya hewa. karatsi Joto la joto na uchambuzi wa latitudo kutabiri kuenea kwa uwezo na msimu wa COVID-19 inabaini kuwa ugonjwa huo umeenea katika hali ya hewa kali (kwa bahati mbaya kwetu, kiwango cha joto huko San Francisco, tunakoishi, ni sawa katika safu hiyo ; pia inashughulikia vituo vikuu vya watu Ulaya, pamoja na London.) . “Usiogope. Tulia.” Haifai . Jibu moja la kawaida ambalo tumeona kwenye media ya kijamii kwa watu ambao wanaonyesha sababu za kuwa na wasiwasi, ni “usiogope” au “tulia”. Hii ni, kusema kidogo, sio msaada. Hakuna mtu anayependekeza kwamba hofu ni majibu sahihi. Kwa sababu fulani, hata hivyo, “utulivu” ni mwitiko maarufu katika duru fulani (lakini sio kati ya wataalam wa magonjwa yoyote, ambao kazi yao ni kufuatilia mambo haya). Labda “kutuliza” husaidia watu wengine kuhisi vizuri juu ya kutotenda kwao, au inawafanya wajisikie bora kwa watu ambao wanafikiria wanakimbiza kama kuku wasio na kichwa. . Lakini “kutuliza” kunaweza kusababisha urahisi kutofanikiwa kuandaa na kujibu. Huko Uchina, mamilioni ya waliwekwa chini na hospitali mbili mpya zilijengwa wakati walipofikia takwimu ambazo Amerika iko sasa. Italia ilisubiri muda mrefu sana, na leo tu (Jumapili Machi 8) waliripoti kesi mpya 1492 na vifo vipya 133, licha ya kuwafungia watu milioni 16. Kulingana na habari bora zaidi ambayo tunaweza kujua katika hatua hii, wiki mbili tu zilizopita Italia ilikuwa katika nafasi ile ile ambayo Amerika na Uingereza ziko leo (kwa njia ya takwimu za maambukizo). . Kumbuka kuwa karibu kila kitu kuhusu covid-19 katika hatua hii iko angani. Hatujui ni kasi ya maambukizi au vifo, hatujui ni muda gani inafanya kazi kwenye nyuso, hatujui ikiwa inaendelea kuishi na kusambaa katika hali ya joto. Kila kitu tulichonacho ni nadhani bora za sasa kulingana na habari bora watu wanaoweza kuweka pamoja. Kumbuka, idadi kubwa ya habari hii iko Uchina, kwa Kichina. Hivi sasa, njia bora ya kuelewa uzoefu wa Wachina hadi sasa ni kusoma Ripoti bora ya Ujumbe wa Pamoja wa WHO-China kuhusu ugonjwa wa Coronavirus 2019, kwa msingi wa dhamira ya pamoja ya wataalam 25 wa kitaifa na kimataifa kutoka Uchina, Ujerumani, Japan, Korea, Nigeria, Urusi, Singapore, Amerika ya Amerika na Shirika la Afya Duniani (WHO). . Wakati hakuna shaka, kwamba labda hii haitakuwa janga la ulimwengu, na labda kila kitu kinaweza kupita bila mfumo wa hospitali kuanguka, hiyo haimaanishi kuwa majibu sahihi sio kufanya chochote. Hiyo inaweza kuwa ya kufikiria sana na sio majibu sahihi chini ya hali yoyote ya mfano wa vitisho. Inaonekana pia kuwa uwezekano mkubwa kuwa nchi kama Italia na Uchina zinaweza kuziba sehemu kubwa za uchumi wao bila sababu nzuri. Sio pia sawa na athari halisi tunayoona ardhini katika maeneo yaliyoambukizwa, ambapo mfumo wa matibabu hauwezi kuhimili (kwa mfano, Italia inatumia hema 462 kwa “utangulizi”, na bado inabidi ihama wagonjwa wa ICU kutoka kwa maeneo yaliyoambukizwa). . Badala yake, mwitikio mzuri, wenye busara ni kufuata hatua ambazo zinapendekezwa na wataalam kuzuia kueneza maambukizo: . Epuka vikundi vikubwa na umati wa watu . | Ghairi matukio . | Fanya kazi kutoka nyumbani, ikiwa inawezekana . | Osha mikono unapokuja na kutoka nyumbani, na mara kwa mara wakati uko nje . | Epuka kugusa uso wako, haswa wakati uko nje ya nyumba yako (sio rahisi!) . | Disin uso na vifurushi (inawezekana virusi vinaweza kubaki kazi kwa siku 9 kwenye nyuso, ingawa hii bado haijajulikana kwa njia yoyote ile). . | . Sio juu yako tu . Ikiwa uko chini ya miaka 50, na hauna sababu za hatari kama mfumo wa kinga uliodhoofishwa, ugonjwa wa moyo na mishipa, historia ya uvutaji wa sigara uliopita, au magonjwa mengine sugu, basi unaweza kuwa na faraja kuwa covid-19 haiko uwezekano wa kukuua. Lakini jinsi unavyojibu bado ni mambo mengi sana. Bado una nafasi kubwa tu ya kuambukizwa, na ikiwa unafanya, nafasi kubwa tu ya kuambukiza wengine. Kwa wastani, kila mtu aliyeambukizwa anaambukiza zaidi ya watu wawili, na wanaambukiza kabla ya kuonyesha dalili. Ikiwa una wazazi unaowajali, au babu na babu, na unapanga kutumia wakati pamoja nao, na baadaye gundua kuwa una jukumu la kuwaambukiza na covid-19, hiyo itakuwa mzigo mzito kuishi nayo. . Hata ikiwa haukuwasiliana na watu zaidi ya miaka 50, kuna uwezekano kuwa una wafanyikazi wengi zaidi na marafiki wako wa kawaida na magonjwa sugu kuliko vile unavyogundua. Utafiti unaonyesha kuwa watu wachache huonyesha hali zao za kiafya mahali pa kazi ikiwa wanaweza kuizuia, kwa hofu ya kubaguliwa. Wote wawili wako katika jamii zilizo katika hatari kubwa, lakini watu wengi ambao tunaingiliana nao mara kwa mara wanaweza wasijue hii. . Na kwa kweli, sio tu juu ya watu wanaokuzunguka mara moja. Hili ni suala muhimu sana la maadili. Kila mtu anayefanya bidii yao kuchangia kudhibiti kuenea kwa virusi ni kusaidia jamii yao yote kupunguza kasi ya maambukizi. Kama Zeynep Tufekci aliandika katika Scientific American: “Kujiandaa kwa kuenea kwa uwezekano wa kuenea kwa virusi vya ulimwengu huu … ni moja wapo ya mambo ya kijamii, ya kujitolea ambayo unaweza kufanya”. Anaendelea: . Tunapaswa kuandaa, sio kwa sababu tunaweza kuhisi kibinafsi, lakini ili tuweze kusaidia kupunguza hatari kwa kila mtu. Tunapaswa kutayarisha sio kwa sababu tunakabiliwa na hali ya siku ya mwisho nje ya uwezo wetu, lakini kwa sababu tunaweza kubadilisha kila kipengele cha hatari hii tunayokabili kama jamii. Hiyo ni kweli, unapaswa kujiandaa kwa sababu majirani zako wanahitaji utayarishe-haswa majirani wako wazee, majirani zako wanaofanya kazi hospitalini, majirani zako walio na magonjwa sugu, na majirani ambao wanaweza kukosa njia au wakati wa kuandaa kwa sababu ya kukosa rasilimali au wakati. . Hii imeathiri sisi kibinafsi. Kozi kubwa na muhimu zaidi ambayo tumewahi kuunda fast.ai, ambayo inawakilisha mwisho wa miaka ya kazi kwa ajili yetu, ilipangwa kuanza katika Chuo Kikuu cha San Francisco katika wiki. Jumatano iliyopita (Machi 4), tulifanya uamuzi wa kuhamisha kitu hicho mkondoni. Tulikuwa moja ya kozi kubwa ya kwanza kuhamia mkondoni. Kwa nini tulifanya? Kwa sababu tuligundua mapema wiki iliyopita kwamba ikiwa tungeendesha kozi hii, tunawahimiza sana mamia ya watu kuungana katika nafasi iliyofungwa, mara kadhaa kwa kipindi cha wiki nyingi. Kuleta vikundi pamoja katika nafasi zilizofungwa ni jambo moja mbaya zaidi ambalo linaweza kufanywa. Tulihisi tulazimishwa kiakili kuhakikisha kuwa, angalau katika kesi hii, hii haikutokea. Ilikuwa uamuzi wa kuvunja moyo. Wakati wetu uliotumiwa kufanya kazi moja kwa moja na wanafunzi wetu imekuwa moja ya starehe nzuri na vipindi vingi vya uzalishaji kila mwaka. Na tulikuwa na wanafunzi waliopanga kuruka kutoka ulimwenguni kote, ambao hatutaki kumruhusu[3]. . Lakini tulijua ni jambo sahihi kufanya, kwa sababu sivyo tutaweza kuwa tunazidisha kuenea kwa ugonjwa huo katika jamii yetu[4]. . Tunahitaji kueneza Curve . Hii ni muhimu sana, kwa sababu ikiwa tunaweza kupunguza kasi ya maambukizi katika jamii, basi tunawapa hospitali katika jamii hiyo wakati wa kukabiliana na wagonjwa wote walioambukizwa, na mzigo wa kawaida wa wagonjwa wanaohitaji kushughulikia. Hii inaelezewa kama “kubatilisha curve”, na imeonyeshwa wazi katika chati hii ya kuonyesha: . Kukaa chini ya mstari huo ulio na alama kunamaanisha kila kitu . . Farzad Mostashari, Mratibu wa Zamani wa Afya ya IT, alielezea: “Kesi mpya zinatambuliwa kila siku ambazo hazina historia ya kusafiri au kiunga cha kesi inayojulikana, na tunajua kuwa hizi ni ncha za barafu kwa sababu ya ucheleweshaji katika kupima. Hiyo inamaanisha kuwa katika wiki mbili zijazo idadi ya watu wanaotambuliwa italipuka…. Kujaribu kufanya jambo wakati kuna kuenea kwa jamii ni kama kuzingatia kuweka cheche wakati nyumba iko moto. Wakati hiyo ikifanyika, tunahitaji kubadili mikakati ya kupunguza - kuchukua hatua za kinga ili polepole kuenea na kupunguza athari za kilele kwa huduma ya afya. “ Ikiwa tunaweza kuweka kuenea kwa magonjwa chini ya kwamba hospitali zetu zinaweza kushughulikia mzigo, basi watu wanaweza kupata matibabu. Lakini ikiwa kesi zinakuja haraka sana, basi zile ambazo zinahitaji kulazwa hazitaipata. . Hii ndio hesabu inaweza kuonekana, kulingana na Liz Specht: . Amerika ina vitanda takriban 2.8 vya hospitali kwa kila watu 1000. Pamoja na idadi ya watu 330M, hii ni vitanda vya ~ 1M. Kwa wakati wowote, 65% ya vitanda hivyo vimeshaa. Hiyo inaacha vitanda 330k vinavyopatikana kote nchini (labda kidogo wakati huu wa mwaka na msimu wa homa ya kawaida, nk). Wacha tuamini idadi ya Italia na kudhani kuwa karibu 10% ya kesi ni kubwa za kutosha kuhitaji kulazwa hospitalini. (Kumbuka kwamba kwa wagonjwa wengi, kulazwa hospitalini kwa wiki - kwa maneno mengine, mauzo yatakuwa polepole sana kwani vitanda hujaza wagonjwa wa COVID19). Kwa makisio haya, kufikia Mei 8, vitanda vyote vya wazi vya hospitali nchini Amerika vitajazwa. (Hii haisemi chochote, kwa kweli, juu ya ikiwa vitanda hivi vinafaa kutengwa kwa wagonjwa walio na virusi vya kuambukiza sana.) Ikiwa tunakosea kwa sababu ya mbili kuhusu sehemu ya kesi kali, hiyo inabadilisha tu ratiba ya muda wa kueneza kitanda. kwa siku 6 kwa pande zote mbili. Ikiwa 20% ya kesi zinahitaji kulazwa hospitalini, tunamaliza vitanda na ~ 2 Mei Ikiwa tu 5% ya kesi zinahitaji, tunaweza kuifanya hadi ~ Mei 14. 2.5% inatupeleka Mei 20. Hii, kwa kweli, inadhani kwamba hakuna uvumbuzi wa mahitaji ya vitanda kutoka kwa sababu zingine (zisizo za COVID19), ambayo inaonekana kama dhana mbaya. Wakati mfumo wa utunzaji wa afya unazidi kuwa mzito, uhaba wa Rx, nk, watu w / hali sugu ambazo husimamiwa vizuri huweza kujikuta wakitumbukia katika majimbo makali ya dhiki ya kitabibu inayohitaji utunzaji mkubwa na kulazwa hospitalini. . Mwitikio wa jamii hufanya tofauti zote . Kama tulivyojadili, hesabu hii sio ukweli- Uchina tayari imeonyesha kuwa inawezekana kupunguza kuenea kwa kuchukua hatua kali. Mfano mwingine mzuri wa mwitikio uliofanikiwa ni Vietnam, ambapo, kati ya mambo mengine, kampeni ya matangazo ya kitaifa (pamoja na wimbo wa kuvutia!) Ilichochea majibu ya jamii haraka na kuhakikisha kuwa watu wanabadilisha tabia zao ipasavyo. . Hii sio hali ya kiakili tu - ilionyeshwa wazi katika janga la mafua la 1918. Huko Merika miji miwili ilionyesha athari tofauti sana kwa janga hili: Philadelphia ilienda mbele na gwaride kubwa la watu 200,000 kusaidia kuongeza pesa kwa vita. Lakini St Louis aliweka michakato iliyoundwa kwa uangalifu ili kupunguza mawasiliano ya kijamii ili kupungua kwa kuenea kwa virusi, pamoja na kufuta hafla zote kubwa. Hapa ndivyo idadi ya vifo ilionekana katika kila mji, kama inavyoonyeshwa kwenye Taratibu za National Academy of Sciences: . Matokeo ya majibu tofauti kwa janga la mafua ya 1918 . . Hali ya Philadelphia ilizidi kuwa mbaya sana, hata kufikia mahali ambapo hakukuwa na pakiti za mazishi za kutosha au mikato ya kushughulikia idadi kubwa ya waliokufa kutokana na homa hiyo. . Richard Besser, ambaye alikuwa kaimu mkurugenzi wa Vituo vya Kudhibiti na Kuzuia Magonjwa wakati wa janga la H1N1 la 2009, anasema kwamba nchini Merika “hatari ya kufichuliwa na uwezo wa kujilinda na familia ya mtu inategemea mapato, ufikiaji wa huduma za afya, na hali ya uhamiaji, miongoni mwa sababu zingine. “ Anaonyesha kuwa: . Wazee na walemavu wako katika hatari fulani wakati maisha yao ya kila siku na mifumo ya msaada inavurugika. Wale wasio na huduma rahisi ya huduma za afya, pamoja na jamii za vijijini na Native, wanaweza kukabiliwa na umbali mzito wakati wa shida. Watu wanaoishi katika nyumba za karibu - iwe katika makazi ya umma, nyumba za wauguzi, magereza, malazi au hata wasio na makazi mitaani - wanaweza kuteseka kwa mawimbi, kama vile tumeona tayari katika jimbo la Washington. Na udhaifu wa uchumi wa gge wa mshahara wa chini, na wafanyikazi wasio na mishahara na ratiba za kazi ngumu, utafunuliwa kwa wote kuona wakati wa shida hii. Uliza asilimia 60 ya nguvu kazi ya Merika ambayo hulipwa saa moja jinsi ni rahisi kuchukua wakati katika hitaji. . Ofisi ya Takwimu ya Kazi ya Amerika inaonyesha kwamba chini ya theluthi ya wale walio kwenye bendi ya kipato cha chini wanapata likizo ya mgonjwa ya kulipwa: . Wamarekani wengi masikini hawana likizo ya kuugua, kwa hivyo lazima uende kazini. . . Hatuna habari nzuri huko Amerika . Mojawapo ya maswala makubwa nchini Merika ni kwamba upimaji mdogo sana unafanywa, na matokeo ya upimaji hayashirikiwi vizuri, ambayo inamaanisha hatujui kile kinachotokea. Scott Gottlieb, kamishna wa zamani wa FDA, alielezea kwamba huko Seattle kumekuwa na majaribio bora, na tunaona maambukizo huko: “Sababu ya sisi kujua mapema juu ya kuzuka kwa Seattle ya covid-19 ni kwa sababu ya kazi ya uchunguzi wa wanasayansi huru. Uchunguzi kama huo haujawahi kuendelea kabisa katika miji mingine. Kwa hivyo maeneo mengine moto ya Merika bado hayawezi kugunduliwa. “ Kulingana na The Atlantic, Makamu wa Rais, Mike Pence aliahidi kwamba “takriban vipimo milioni 1.5” vitapatikana wiki hii, lakini watu wasiopungua 2000 wamejaribiwa kote Amerika kwa sasa. Kuchora kazi kutoka Mradi wa COVID Tracking Project, Robinson Meyer na Alexis Madrigal wa Atlantic, walisema: . Takwimu tulizokusanya zinaonyesha kuwa mwitikio wa Amerika kwa covid-19 na ugonjwa unaosababisha, COVID-19, imekuwa ya uvivu wa kushangaza, haswa ikilinganishwa na ile ya nchi nyingine zilizoendelea. CDC ilithibitisha siku nane zilizopita kwamba virusi hivyo vilikuwa katika maambukizi ya jamii huko Merika-kwamba ilikuwa ikiambukiza Wamarekani ambao walikuwa hawajasafiri kwenda nje ya nchi na hawakuwasiliana na wengine ambao walikuwa. Huko Korea Kusini, zaidi ya watu 66,650 walijaribiwa katika wiki moja ya kesi ya kwanza ya maambukizi ya jamii, na mara moja iliweza kujaribu watu 10,000 kwa siku. . Sehemu ya shida ni kwamba hii imekuwa suala la kisiasa. Hasa, Rais Donald Trump ni mfano wa ambapo utaftaji wa madini unaingilia kati kupata matokeo mazuri katika mazoezi. (Kwa zaidi juu ya suala hili, tazama Maadili ya Karatasi ya Sayansi ya Tatizo Shida na Metric ni Tatizo la AI kwa msingi). Mkuu wa Google wa AI Jeff Dean, tweeted juu ya shida za disinformation za kisiasa: . Wakati nilifanya kazi kwa WHO, nilikuwa sehemu ya Programu ya Ulimwenguni juu ya UKIMWI (sasa UNAIDS), iliyoundwa ili kusaidia ulimwengu kukabiliana na janga la VVU / UKIMWI. Wafanyikazi hapo walikuwa madaktari na wanasayansi waliojitolea sana kulenga kushughulikia shida hiyo. Katika nyakati za shida, habari wazi na sahihi ni muhimu kusaidia kila mtu kufanya maamuzi sahihi na yenye habari juu ya jinsi ya kujibu (nchi, serikali, na serikali za mitaa, kampuni, NGO, shule, familia, na watu binafsi). Pamoja na habari na sera sahihi zilizowekwa kwa ajili ya kusikiliza wataalam bora wa matibabu na wanasayansi, sote tutakuja kupitia changamoto kama zile zilizowasilishwa na VVU / UKIMWI au COVID-19. Pamoja na utaftaji unaoendeshwa na masilahi ya kisiasa, kuna hatari ya kweli ya kufanya mambo kuwa mbaya zaidi, kwa kutofanya haraka na kwa dhati mbele ya janga linalokua, na kwa kutia moyo tabia ambazo zitasambaza ugonjwa haraka. Hali hii nzima ni chungu sana kutazama ikitokea. . Haionekani kana kwamba kuna matakwa ya kisiasa ya kugeuza mambo, inapofikia uwazi. Katibu wa Huduma za Afya na Binadamu, Alex Azar, kulingana na Wired, “alianza kuzungumza juu ya vipimo ambavyo wafanyikazi wa huduma ya afya hutumia kubaini ikiwa mtu ameambukizwa na ugonjwa mpya. Ukosefu wa vifaa hivyo unamaanisha ukosefu mkubwa wa habari ya ugonjwa kuhusu kuenea na ukali wa ugonjwa huo huko Merika, uliozidishwa na opacity kwa upande wa serikali. Azar alijaribu kusema kuwa majaribio zaidi yalikuwa njiani, yanasubiri udhibiti wa ubora. “ Lakini, waliendelea: . Kisha Trump akamkata Azar. “Lakini nadhani, muhimu, mtu yeyote, hivi sasa na jana, kwamba anahitaji mtihani anapata mtihani. Wapo, wana vipimo, na vipimo ni nzuri. Mtu yeyote anayehitaji mtihani anapata mtihani, “Trump alisema. Huo sio ukweli. Makamu wa Rais Pence aliwaambia waandishi wa habari Alhamisi kuwa Amerika haina vifaa vya kutosha vya kukidhi mahitaji. . Nchi zingine zinajibu haraka sana na kwa kiwango kikubwa kuliko Amerika. Nchi nyingi katika SE Asia zinaonyesha matokeo mazuri, pamoja na Taiwan, ambapo R0 iko chini hadi 0.3 sasa, na Singapore, ambayo inapendekezwa kama The Model for COVID-19 Response. Sio tu kwa Asia; kwa Ufaransa, kwa mfano, mkusanyiko wowote wa watu 1000 ni marufuku, na shule sasa zimefungwa katika wilaya tatu. . Hitimisho . Covid-19 ni suala muhimu la kijamii, na tunaweza, na tunapaswa, wote kufanya kazi kupungua kwa kuenea kwa ugonjwa huo. Hii inamaanisha: . Kuepuka vikundi vikubwa na umati wa watu . | Inaghairi matukio . | Kufanya kazi kutoka nyumbani, ikiwa inawezekana . | Kuosha mikono wakati unakuja na kutoka nyumbani, na mara kwa mara wakati uko nje . | Kuepuka kugusa uso wako, haswa ukiwa nje ya nyumba yako. . | . Kumbuka: kwa sababu ya dharura ya kupata hayo, tumekuwa sio waangalifu kama kawaida tunapenda kuwa juu ya kuhutubia na kuashiria kazi tunayotegemea. Tafadhali tujulishe ikiwa tumekosa chochote. . Asante kwa Sylvain Gugger na Alexis Gallagher kwa maoni na maoni. . Maelezo ya chini . 1. Epidemiologists ni watu ambao husoma kuenea kwa magonjwa. Inabadilika kuwa makadirio ya vitu kama vifo na R0 ni changamoto nzuri kweli, kwa hivyo kuna uwanja mzima ambao utaalam katika kufanya hivi vizuri. Kuwa mwangalifu na watu ambao hutumia uhesabu rahisi na takwimu kukuambia jinsi covid-19 inavyoendelea. Badala yake, angalia mfano unaofanywa na wataalam wa magonjwa ya magonjwa . 2. Kweli, sio kweli kitaalam. “R0” kuzungumza madhubuti inahusu kiwango cha maambukizi kwa kukosekana kwa majibu. Lakini kwa kuwa hiyo sio jambo ambalo tunawajali sana, tunajiruhusu kuwa kidogo wepesi juu ya ufafanuzi wetu hapa . 3. Tangu uamuzi huo, tumejitahidi kupata njia ya kuendesha kozi halisi ambayo tunatumai itakuwa bora zaidi kuliko toleo la mtu mwenyewe ingekuwa. Tumeweza kuifungua kwa mtu yeyote ulimwenguni, na tutakuwa tukifanya uchunguzi wa kawaida na vikundi vya miradi kila siku . 4. Tumefanya mabadiliko mengine madogo kwa mtindo wetu wa maisha pia, pamoja na mazoezi nyumbani badala ya kwenda kwenye mazoezi, kusonga mikutano yetu yote kwenye mkutano wa video, na kuruka hafla za usiku ambazo tumekuwa tukitazamia .",
            "url": "https://asvcode.github.io/Blogs/coronavirus/covid19/sawhili/kiswahili/2020/03/10/Covid-19-jamii-yako-na-wewe-mtazamo-wa-sayansi-ya-data.html",
            "relUrl": "/coronavirus/covid19/sawhili/kiswahili/2020/03/10/Covid-19-jamii-yako-na-wewe-mtazamo-wa-sayansi-ya-data.html",
            "date": " • Mar 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Test",
            "content": "What are Dicom images? . DICOM(Digital Imaging and COmmunications in Medicine) is the de-facto standard that establishes rules that allow medical images(X-Ray, MRI, CT) and associated information to be exchanged between imaging equipment from different vendors, computers, and hospitals. The DICOM format provides a suitable means that meets health infomation exchange (HIE) standards for transmision of health related data among facilites and HL7 standards which is the messaging standard that enables clinical applications to exchange data. . DICOM files typically have a .dcm extension and provides a means of storing data in seperate &#39;tags&#39; such as patient information as well as image/pixel data. A DICOM file consists of a header and image data sets packed into a single file. The information within the header is organized as a constant and standardized series of tags. By extracting data from these tags one can access important information regarding the patient demographics, study parameters, etc . 16 bit DICOM images have values ranging from -32768 to 32768 while 8-bit greyscale images store values from 0 to 255. The value ranges in DICOM images are useful as they correlate with the Hounsfield Scale which is a quantitative scale for describing radiodensity . Parts of a DICOM .",
            "url": "https://asvcode.github.io/Blogs/test/2020/01/01/Test.html",
            "relUrl": "/test/2020/01/01/Test.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This site is built with fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. . . fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. A full list of features can be found on GitHub. .",
          "url": "https://asvcode.github.io/Blogs/fastpages/",
          "relUrl": "/fastpages/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}